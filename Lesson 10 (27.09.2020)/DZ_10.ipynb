{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DZ-10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMzSltDxJbm_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "bae2ef4a-19f6-4af5-ce49-0c9c4e8c895e"
      },
      "source": [
        "!pip install imageio\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DeyQUN_bpRG"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "from scipy.stats import norm\n",
        "import sys\n",
        "import time\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "from keras.datasets import mnist \n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import BatchNormalization, Dropout, Flatten, Reshape, Lambda, Input, Dense, concatenate, Conv2D, Conv2DTranspose, InputLayer  \n",
        "from keras.models import Model, Sequential\n",
        "from keras.objectives import binary_crossentropy\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from IPython.display import clear_output\n",
        "from keras.callbacks import LambdaCallback, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjBYR5xVcsHO"
      },
      "source": [
        "LIGHT - 1(Работает очень медленно, результат отображение картинок)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTOg6b5SLKqT"
      },
      "source": [
        "# загружаем данные из базы mnist с разбивкой на train/test\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test  = x_test .astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test  = np.reshape(x_test,  (len(x_test),  28, 28, 1))\n",
        "\n",
        "x_train[x_train >= .5] = 1.\n",
        "x_train[x_train < .5] = 0.\n",
        "\n",
        "x_test[x_test >= .5] = 1.\n",
        "x_test[x_test < .5] = 0.\n",
        "\n",
        "y_train_cat = to_categorical(y_train).astype(np.float32)\n",
        "y_test_cat  = to_categorical(y_test).astype(np.float32)\n",
        "num_classes = y_test_cat.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGouz7W4LVgX"
      },
      "source": [
        "TRAIN_SIZE = 60000\n",
        "BATCH_SIZE = 50\n",
        "TEST_SIZE = 10000\n",
        "train_batch = tf.data.Dataset.from_tensor_slices(x_train).shuffle(TRAIN_SIZE).batch(BATCH_SIZE)\n",
        "test_batch = tf.data.Dataset.from_tensor_slices(x_test).shuffle(TEST_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtCQMVxSLyxl"
      },
      "source": [
        "class CONV_VAE(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(CONV_VAE, self).__init__()\n",
        "    self.latent_dim = latent_vec\n",
        "# ENCODER\n",
        "    self.encoder_model = Sequential([InputLayer(input_shape=(28, 28, 1)),\n",
        "                                     Conv2D(filters=25, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "                                     Conv2D(filters=50, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "                                     Flatten(),\n",
        "                                     Dense(latent_vec + latent_vec),\n",
        "                                     ])  \n",
        "# DECODER\n",
        "    self.decoder_model = Sequential([InputLayer(input_shape=(latent_vec,)),\n",
        "                                     Dense(units= 7*7*25, activation=tf.nn.relu),\n",
        "                                     Reshape(target_shape=(7, 7, 25)),\n",
        "                                     Conv2DTranspose(filters=50, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'),\n",
        "                                     Conv2DTranspose(filters=25, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'),\n",
        "                                     Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\"),])\n",
        "    \n",
        "  @tf.function\n",
        "  def sampling(self, sam=None):\n",
        "    if sam is None:\n",
        "      sam = tf.random.normal(shape=(50, self.latent_vec))\n",
        "    return self.decoder(sam, apply_sigmoid=True)\n",
        "\n",
        "  def encoder(self, inp):\n",
        "    mean, logd = tf.split(self.encoder_model(inp), num_or_size_splits=2, axis=1)\n",
        "    return mean, logd\n",
        "\n",
        "  def reparameterization(self, mean, logd):\n",
        "    sam = tf.random.normal(shape=mean.shape)\n",
        "    return sam * tf.exp(logd * .5) + mean\n",
        "\n",
        "  def decoder(self, out, apply_sigmoid=False):\n",
        "    logout = self.decoder_model(out)\n",
        "    if apply_sigmoid:\n",
        "      probabs = tf.sigmoid(logout)\n",
        "      return probabs\n",
        "\n",
        "    return logout\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evRVIHoRQNRM"
      },
      "source": [
        "optimizer_func = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_prob_dist_func(sample, mean, logd, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logd) + logd + log2pi), axis=raxis)\n",
        "\n",
        "@tf.function\n",
        "def loss_func(model, inp):\n",
        "  mean, logd = model.encoder(inp)\n",
        "  out = model.reparameterization(mean, logd)\n",
        "  log_inp = model.decoder(out)\n",
        "  cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=log_inp, labels=inp)\n",
        "  logp_inp_out = -tf.reduce_sum(cross_entropy, axis=[1, 2, 3])\n",
        "  logp_out = log_normal_prob_dist_func(out, 0., 0.)\n",
        "  logq_out_inp = log_normal_prob_dist_func(out, mean, logd)\n",
        "  return -tf.reduce_mean(logp_inp_out + logp_out - logq_out_inp)\n",
        "\n",
        "@tf.function\n",
        "def gradient_func(model, inp, optimizer_func):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = loss_func(model, inp)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer_func.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrBZslBqQNna"
      },
      "source": [
        "epochs = 100\n",
        "latent_vec = 8\n",
        "examples = 8\n",
        "\n",
        "rand_vec = tf.random.normal(\n",
        "    shape=[examples, latent_vec])\n",
        "model = CONV_VAE(latent_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjjVGno8Wl8s"
      },
      "source": [
        "def generate_and_save_images(model, epochs, input_data):\n",
        "  preds = model.sampling(input_data)\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(preds.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(preds[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('img_at_epoch{:04d}.png'.format(epochs))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQlGGvE1WmCj"
      },
      "source": [
        "generate_and_save_images(model, 0, rand_vec)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  start_time = time.time()\n",
        "  for x in train_batch:\n",
        "    gradient_func(model, x, optimizer_func)\n",
        "  end_time = time.time()\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    loss = tf.keras.metrics.Mean()\n",
        "    for y in test_batch:\n",
        "      loss(loss_func(model, y))\n",
        "    elbo = -loss.result()\n",
        "    display.clear_output(wait=False)\n",
        "    print('Epoch no.: {}, Test batch ELBO: {}, '\n",
        "          'elapsed time for current epoch {}'.format(epochs, elbo, end_time - start_time))\n",
        "    generate_and_save_images(model, epochs, rand_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y41cEMGWmI1"
      },
      "source": [
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('img_at_epoch{:04d}.png'.format(epoch_no))\n",
        "\n",
        "plt.imshow(display_image(epochs))\n",
        "plt.axis('off')# Display images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZavOyf5dS5s"
      },
      "source": [
        "PRO 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1UYiev-de0f"
      },
      "source": [
        "!pip install --upgrade --ignore-installed tensorflow\n",
        "!pip install -q imageio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sznuOV_cdfDJ"
      },
      "source": [
        "#Подключаем библиотеки\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import concatenate, Input, Dense, Dropout, BatchNormalization, Flatten, Conv1D, Conv2D, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras import utils\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4tOWs-hdfGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a01e1ae4-ff41-485a-b75a-30f3b63d3837"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm-Tv0CGdfJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f444430d-3b12-4509-9127-f5b1f22485fb"
      },
      "source": [
        "import zipfile #Для разорхивации базы\n",
        "\n",
        "#Разорхивируем архив с базой\n",
        "z = zipfile.ZipFile('/content/drive/My Drive/base/genres.zip', 'r')\n",
        "z.extractall()\n",
        "\n",
        "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
        "\n",
        "#Проверяем выгруженные папки\n",
        "!ls genres \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blues  classical  country  disco  hiphop  jazz\tmetal  pop  reggae  rock\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uca80Y0ve3AO"
      },
      "source": [
        "#Функция параметризации аудир\n",
        "def get_features(y, sr):\n",
        "  #Получаем различные параметры аудио\n",
        "  chroma_stft = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n",
        "  rmse = np.mean(librosa.feature.rmse(y=y))\n",
        "  spec_cent = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "  spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
        "  rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "  zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
        "  mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "  \n",
        "  #Добавляем все параметры в один список\n",
        "  out = []\n",
        "  out.append(chroma_stft)\n",
        "  out.append(rmse)\n",
        "  out.append(spec_cent)\n",
        "  out.append(spec_bw)\n",
        "  out.append(rolloff)\n",
        "  out.append(zcr)\n",
        "  \n",
        "  #По одному добавляем все Мел коэффициенты\n",
        "  for e in mfcc:\n",
        "    out.append(np.mean(e))\n",
        "  \n",
        "  #Возвращаем получившийся список\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XOTP_bWe3Dm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "23289464-9587-4adb-ad15-660eefbdd0f9"
      },
      "source": [
        "#Формируем обучающую выборку\n",
        "#Создаём пустые листы\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "#Запоминаем время старта формирования выборки\n",
        "curr_time = time.time()\n",
        "\n",
        "#Проходим по всем жарнам\n",
        "for i in range(len(genres)):\n",
        "  g = genres[i] \n",
        "  for filename in os.listdir(f'./genres/{g}'):\n",
        "    songname = f'./genres/{g}/{filename}'\n",
        "    y, sr = librosa.load(songname, mono=True, duration=30)\n",
        "    out = get_features(y, sr)\n",
        "\n",
        "    X_train.append(out)\n",
        "    Y_train.append(utils.to_categorical(i, len(genres)))\n",
        "\n",
        "  print(\"Жанр \", g, \" готов -> \", round(time.time() - curr_time), \"c\", sep=\"\")\n",
        "  curr_time = time.time()\n",
        "\n",
        "#Превращаем обучающую выборку на numpy массивы\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Жанр blues готов -> 110c\n",
            "Жанр classical готов -> 107c\n",
            "Жанр country готов -> 111c\n",
            "Жанр disco готов -> 109c\n",
            "Жанр hiphop готов -> 106c\n",
            "Жанр jazz готов -> 109c\n",
            "Жанр metal готов -> 108c\n",
            "Жанр pop готов -> 106c\n",
            "Жанр reggae готов -> 110c\n",
            "Жанр rock готов -> 105c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn4e-GAue3Gx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f6c8774d-b0a3-46a4-c14c-1e51462aed19"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 26)\n",
            "(1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fGevm46e3LG"
      },
      "source": [
        "#Создаём backup обучающей выборки\n",
        "X_train_backup = X_train.copy()\n",
        "Y_train_backup = Y_train.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S05We5YQe3Op",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "bfe764d2-b404-4cac-f672-c8ab62692011"
      },
      "source": [
        "y_train_class = np.argmax(Y_train, axis=1)\n",
        "X_train_backup = np.array(X_train_backup)\n",
        "print(y_train_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
            " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7\n",
            " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
            " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
            " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
            " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
            " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
            " 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct07PZ6qjSko"
      },
      "source": [
        "def load(file_):\n",
        "    data_, sampling_rate = librosa.load(file_,sr=3000, offset=0.0, duration=30)\n",
        "    data_ = data_.reshape(1,90001)\n",
        "    return data_\n",
        "map_data = lambda filename: tf.compat.v1.py_func(load, [filename], [tf.float32])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx6L3NzMh4R6"
      },
      "source": [
        "train_size = 60000\n",
        "BATCH_SIZE = 8\n",
        "test_size = 10000\n",
        "epochs = 20\n",
        "latent_dim = 2\n",
        "num_examples_to_generate = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONC7KRKJhv2R"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices((X_train_backup)).map(map_data, num_parallel_calls=AUTOTUNE).shuffle(3).batch(BATCH_SIZE))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices((Y_train_backup)).map(map_data, num_parallel_calls=AUTOTUNE).shuffle(3).batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fiFl_0lilNa"
      },
      "source": [
        "class CVAE(Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = Sequential(\n",
        "            [InputLayer(input_shape=(1,90001)),\n",
        "            Conv1D(64,1,2),\n",
        "            Conv1D(128,1,2),\n",
        "            Conv1D(128,1,2),\n",
        "            Conv1D(256,1,2),\n",
        "            Flatten(),\n",
        "            Dense(latent_dim+latent_dim)\n",
        "            ])\n",
        "        self.decoder = tf.keras.Sequential(\n",
        "            [InputLayer(input_shape=(latent_dim,)),\n",
        "            Reshape(target_shape=(1,latent_dim)),\n",
        "            Conv1DTranspose(512,1,1),\n",
        "            Conv1DTranspose(256,1,1),\n",
        "            Conv1DTranspose(128,1,1),\n",
        "            Conv1DTranspose(64,1,1),\n",
        "            Conv1DTranspose(90001,1,1),\n",
        "            ]\n",
        "        )\n",
        "    @tf.function\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            eps = tf.random.normal(shape=(200, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True)\n",
        "    @tf.function\n",
        "    def encode(self, x):\n",
        "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "    @tf.function\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    @tf.function\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.decoder(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-mE2hH6k-9F"
      },
      "source": [
        "optimizer = Adam(0.0003,beta_1=0.9, beta_2=0.999,epsilon=1e-08)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FitbZCrulAOZ"
      },
      "source": [
        "@tf.function\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "    log2pi = tf.math.log(2. * np.pi)\n",
        "    return tf.reduce_sum(\n",
        "         -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "          axis=raxis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm8OoTN_lARZ"
      },
      "source": [
        "@tf.function\n",
        "def compute_loss(model, x):\n",
        "    mean, logvar = model.encode(x)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2])\n",
        "    logpz = log_normal_pdf(z, 0., 0.)\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytj7g5JElAZ_"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "            mean, logvar = model.encode(x)\n",
        "            z = model.reparameterize(mean, logvar)\n",
        "            x_logit = model.decode(z)\n",
        "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "            logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2])\n",
        "            logpz = log_normal_pdf(z, 0., 0.)\n",
        "            logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "            loss_KL = -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                     tf.keras.losses.binary_crossentropy(x, x_logit)\n",
        "                 )\n",
        "            total_loss = reconstruction_loss+ loss_KL\n",
        "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlBIedQglAXi"
      },
      "source": [
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
        "\n",
        "random_vector_for_generation = tf.random.normal(\n",
        "    shape=[num_examples_to_generate, latent_dim])\n",
        "model = CVAE(latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r_nhCHGlAUi"
      },
      "source": [
        "import librosa.display\n",
        "save_music=[]\n",
        "def generate_and_save_images(model, epoch, test_sample):\n",
        "    mean, logvar = model.encode(test_sample)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    predictions = model.sample(z)\n",
        "    fig = plt.figure(figsize=(18, 15))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        wave = np.asarray(predictions[i])\n",
        "        if epoch>14:\n",
        "            save_music.append(wave)\n",
        "        librosa.display.waveplot(wave[0], sr=3000)\n",
        "\n",
        "    plt.savefig('test.png'.format(epoch))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tu7xSJinc4s"
      },
      "source": [
        "assert BATCH_SIZE >= num_examples_to_generate\n",
        "for test_batch in test_dataset.take(1):\n",
        "    test_sample = test_batch[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7IW8BwHnbec"
      },
      "source": [
        "generate_and_save_images(model, 0, test_sample)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    start_time = time.time()\n",
        "    for train_x in train_dataset:\n",
        "        train_x = np.asarray(train_x)[0]\n",
        "        train_step(model, train_x, optimizer)\n",
        "    end_time = time.time()\n",
        "\n",
        "    loss = tf.keras.metrics.Mean()\n",
        "    for test_x in test_dataset:\n",
        "        test_x = np.asarray(test_x)[0]\n",
        "        loss(compute_loss(model, test_x))\n",
        "    display.clear_output(wait=False)\n",
        "    elbo = -loss.result()\n",
        "    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'.format(epoch, elbo, end_time - start_time))\n",
        "    generate_and_save_images(model, epoch, test_sample)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}