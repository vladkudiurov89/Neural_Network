# -*- coding: utf-8 -*-
"""DZ-21(Pro-Spark).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JfjpW98HP22zmoaZYtS25q4SK11UUHiR
"""

# Устанавливаем необходимые пакеты
!apt-get update
!apt-get install openjdk-8-jdk-headless

!wget http://mirror.klaus-uwe.me/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
!tar xf spark-2.4.7-bin-hadoop2.7.tgz

!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
!java -version
!ls /usr/lib/jvm/java-8-openjdk-amd64

# Библиотеки для использования SparkDL
!pip uninstall -y keras tensorflow fancyimpute
!pip install keras==2.2.5 tensorflow-gpu==1.12.0 findspark sparkdl \
    tensorframes kafka-python tensorflowonspark

# Устанавливаем CUDA 9.0
!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb
!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb
!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub
!apt-get update
!apt-get install cuda=9.0.176-1

!ls /usr/lib/jvm/java-8-openjdk-amd64

!pip install -q findspark

import os
# Задаем окружение
# Указываем переменные окружения для findspark
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "spark-2.4.7-bin-hadoop2.7"

# Инциализируем pyspark из директории с библиотекой
import findspark
findspark.init()

# Запускаем Spark-сессию
from pyspark.sql import SparkSession
spark = SparkSession.builder.master('local[*]').getOrCreate()

# Подгружаем google-диск
from google.colab import drive 
drive.mount('/content/drive/')

import pandas as pd
# Загружаем датасет с диска
df = pd.read_csv('/content/drive/My Drive/datasets/test_magnit/data_classification_for_task.csv')
display(df)

df.info()

# Преобразуем его в датафрейм spark
data = spark.createDataFrame(df)
data.show()

from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Объединяем все числовые переменные в одну векторную колонку
assembler = VectorAssembler(
    inputCols=['AGE', 'GENDER', 'FEATURE_1', 'FEATURE_2', 'FEATURE_3', 'FEATURE_4', 'FEATURE_5', 'FEATURE_6', 'FEATURE_7', 'FEATURE_8', 'FEATURE_9', 'FEATURE_10', 'FEATURE_11'],
    outputCol = 'Attributes') # создаем объект, преобразующий ряды и столбы в характеристические вектора
output = assembler.transform(data) # преобразуем 
output

# Выбираем X (созданная нами векторная колонка Attributes) и Y(колонка TARGET)
finalized_data = output.select("Attributes", "TARGET")
finalized_data.show(truncate=False)

# Делим данные на train/test 80:20
train_data, test_data = finalized_data.randomSplit([0.8,0.2])

# Импортируем метод RandomForestClassifier
from pyspark.ml.classification import RandomForestClassifier 

# Указываем по каким параметрам будет классификация
RandomForest = RandomForestClassifier(featuresCol = 'Attributes', labelCol = 'TARGET')

# Запускаем обучение
RandomForest1 = RandomForest.fit(train_data)

from pyspark.ml import Pipeline

# Создаем пайплайн(оценщик)
pipeline = Pipeline(stages=[RandomForest1])

# Обучаем модель
model = pipeline.fit(train_data)

# Делаем предсказание тестовых данных
predictions = model.transform(test_data)

# Выведем первые 20 значений
predictions.show(200, truncate=False)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Посчитаем точность модели
evaluator = MulticlassClassificationEvaluator(
    labelCol="TARGET", predictionCol="prediction", metricName='accuracy')
accuracy = evaluator.evaluate(predictions)
print("Точность = %g" % accuracy)

