{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DZ-21(Ultro-1).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMjuiUJZfGY_"
      },
      "source": [
        "!pip uninstall -y keras tensorflow fancyimpute\r\n",
        "!pip install keras==2.2.5 tensorflow-gpu==1.13.1 findspark sparkdl \\\r\n",
        "    tensorframes kafka-python tensorflowonspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shb6yGzkeBO5"
      },
      "source": [
        "!pip install sparkdl "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3I0F1OvfwqT"
      },
      "source": [
        "# Устанавливаем необходимые пакеты\r\n",
        "!apt-get update\r\n",
        "!apt-get install openjdk-8-jdk-headless\r\n",
        "!wget http://mirror.klaus-uwe.me/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\r\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\r\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\r\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E_uHa2_fwx1"
      },
      "source": [
        "!ls /usr/lib/jvm/java-8-openjdk-amd64\r\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLwaf07PlUj7"
      },
      "source": [
        "# Устанавливаем CUDA 9.0\r\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\r\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\r\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\r\n",
        "!apt-get update\r\n",
        "!apt-get install cuda=9.0.176-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLPraA77fw0m"
      },
      "source": [
        "import os\r\n",
        "# Задаем окружение\r\n",
        "# Указываем переменные окружения для findspark\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Hg0JR_f6IV"
      },
      "source": [
        "# Инциализируем pyspark из директории с библиотекой\r\n",
        "import findspark\r\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAq_C1NFhqoC"
      },
      "source": [
        "# Запускаем Spark-сессию\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master('local[*]').config(\"spark.driver.memory\", \"16g\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WA2eyr6eGF-"
      },
      "source": [
        "# Подгружаем google-диск\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i49pTzQ0eSRQ"
      },
      "source": [
        "# Загружаем ключ API\r\n",
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhVemZjXeSUE"
      },
      "source": [
        "# # Загружаем \r\n",
        "# !mkdir -p ~/.kaggle\r\n",
        "# !cp kaggle.json ~/.kaggle/\r\n",
        "# # Изменим разрешение\r\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\r\n",
        "# # Загрузим датасет с Kaggle\r\n",
        "# !kaggle competitions download -c dogs-vs-cats\r\n",
        "\r\n",
        "# \r\n",
        "# !unzip -q train.zip -d .\r\n",
        "# !unzip -q test1.zip -d .\r\n",
        "\r\n",
        "!ls '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyIhvwDciTwk"
      },
      "source": [
        "# Распарсим обучающую выборку на категории\r\n",
        "filenames = os.listdir('/content/train/')\r\n",
        "categories = []\r\n",
        "for filename in filenames:\r\n",
        "    category = filename.split('.')[0]\r\n",
        "    if category == 'dog':\r\n",
        "        categories.append(str(1))\r\n",
        "    else:\r\n",
        "        categories.append(str(0))\r\n",
        "\r\n",
        "# Загружаем пандас для создания таблицы\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.DataFrame({\r\n",
        "    'filename': filenames,\r\n",
        "    'category': categories\r\n",
        "})\r\n",
        "\r\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GflaKb9UiT3m"
      },
      "source": [
        "# Выведем случайное изображение\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.preprocessing.image import load_img\r\n",
        "\r\n",
        "sample = random.choice(filenames)\r\n",
        "image = load_img(\"/content/train/\" + sample)\r\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIvhdJxPe1pr"
      },
      "source": [
        "# Преобразуем его в датафрейм spark\r\n",
        "data = spark.createDataFrame(df)\r\n",
        "data.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnZ1tEYTs5-3"
      },
      "source": [
        "data.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_Ask4Ame1sf"
      },
      "source": [
        "# Проверим статистику\r\n",
        "data.describe([\"filename\",\"category\"]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Hb57lhmwgj"
      },
      "source": [
        "from keras.applications import InceptionV3\r\n",
        "\r\n",
        "model = InceptionV3(weights=\"imagenet\")\r\n",
        "model.save('model-full.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKCTuLEHmwje"
      },
      "source": [
        "from keras.applications.inception_v3 import preprocess_input\r\n",
        "from keras.preprocessing.image import img_to_array, load_img\r\n",
        "import numpy as np\r\n",
        "from pyspark.sql.types import StringType\r\n",
        "from sparkdl import KerasImageFileTransformer\r\n",
        "\r\n",
        "def loadAndPreprocessKerasInceptionV3(data):\r\n",
        "    image = img_to_array(load_img(data, target_size=(299, 299)))\r\n",
        "    image = np.expand_dims(image, axis=0)\r\n",
        "    return preprocess_input(image)\r\n",
        "\r\n",
        "transformer = KerasImageFileTransformer(inputCol=\"filename\", outputCol=\"category\",\r\n",
        "                                        modelFile='model-full.h5',\r\n",
        "                                        imageLoader=loadAndPreprocessKerasInceptionV3,\r\n",
        "                                        outputMode=\"vector\")\r\n",
        "transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv9gkHSInyfm"
      },
      "source": [
        "fs = !ls content/train/*.jpg\r\n",
        "uri_df = spark.createDataFrame(fs, StringType()).toDF(\"filename\")\r\n",
        "keras_pred_df = transformer.transform(uri_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulXbQJU8nyse"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "import numpy as np\r\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, FloatType\r\n",
        "\r\n",
        "num_features = 10\r\n",
        "num_examples = 100\r\n",
        "input_data = [{\"features\" : np.random.randn(num_features).astype(float).tolist()} for i in range(num_examples)]\r\n",
        "schema = StructType([ StructField(\"features\", ArrayType(FloatType()), True)])\r\n",
        "input_df = spark.createDataFrame(input_data, schema)\r\n",
        "\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(units=20, input_shape=[num_features], activation='relu'))\r\n",
        "model.add(Dense(units=1, activation='sigmoid'))\r\n",
        "model_path = \"simple-binary-classification\"\r\n",
        "model.save('model_path')\r\n",
        "\r\n",
        "\r\n",
        "transformer = KerasImageFileTransformer(inputCol=\"features\", outputCol=\"category\",\r\n",
        "                                        modelFile='model_path',\r\n",
        "                                        imageLoader=loadAndPreprocessKerasInceptionV3,\r\n",
        "                                        outputMode=\"vector\")\r\n",
        "final_df = transformer.transform(input_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}