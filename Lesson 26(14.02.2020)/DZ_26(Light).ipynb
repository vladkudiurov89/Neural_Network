{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DZ-26(Light).ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvIGWfokq7EF"
      },
      "source": [
        "# убеждаемся, что используем видеокарту\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITXhqLfzqGIT"
      },
      "source": [
        "# Подключаем библиотеки\r\n",
        "import torch\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1tsS2pXqlCy"
      },
      "source": [
        "# Переключаем cuda\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROlvHl4LqlFj"
      },
      "source": [
        "# Клонируем датасет\r\n",
        "!git clone https://github.com/jaddoescad/ants_and_bees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXAssOrRqlIi"
      },
      "source": [
        "!ls ants_and_bees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuPFoXTdqlLa"
      },
      "source": [
        "# Загружаем датасет\r\n",
        "transform_train = transforms.Compose([transforms.Resize([224,224]),\r\n",
        "                                transforms.RandomHorizontalFlip(),\r\n",
        "                                transforms.RandomAffine(0,shear = 10,scale=(.8,1.2)),\r\n",
        "                                transforms.ColorJitter(brightness =1,saturation = 1,contrast = 1),\r\n",
        "                                transforms.ToTensor(),\r\n",
        "                                transforms.Normalize([.5,],[.5,])])\r\n",
        "\r\n",
        "\r\n",
        "transform = transforms.Compose([transforms.Resize([224,224]),\r\n",
        "                                transforms.ToTensor(),\r\n",
        "                                transforms.Normalize([.5,],[.5,])])\r\n",
        "                                \r\n",
        "\r\n",
        "training_dataset = datasets.ImageFolder('ants_and_bees/train',transform =transform_train)\r\n",
        "validation_dataset = datasets.ImageFolder('ants_and_bees/val',transform =transform)\r\n",
        "\r\n",
        "training_loader = torch.utils.data.DataLoader(training_dataset,batch_size = 20,shuffle =True)\r\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset,batch_size = 20,shuffle =False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Gz9xLBrLok"
      },
      "source": [
        "# Функция для конвертации\r\n",
        "def im_convert(tensor):\r\n",
        "  image = tensor.clone().detach().numpy()\r\n",
        "  image = image.transpose(1,2,0)\r\n",
        "  print(image.shape)\r\n",
        "  image = image*np.array([.5,.5,.5])+np.array([.5,.5,.5])\r\n",
        "  image = image.clip(0,1)\r\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt2NaKCArLr_"
      },
      "source": [
        "# выводим картинки\r\n",
        "classes = ('ant', 'bee')\r\n",
        "dataiter  = iter(training_loader)\r\n",
        "images,labels = dataiter.next()\r\n",
        "fig = plt.figure(figsize=(25,4))\r\n",
        "\r\n",
        "for idx in np.arange(20):\r\n",
        "  ax = fig.add_subplot(2,10,idx+1,xticks=[],yticks=[])\r\n",
        "  plt.imshow(im_convert(images[idx]))\r\n",
        "  ax.set_title(classes[labels[idx].item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8O7ZBzrLvN"
      },
      "source": [
        "# загружаем модель\r\n",
        "model = models.vgg16(pretrained = True)\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n9HCbTArYN4"
      },
      "source": [
        "# Указываем параметры\r\n",
        "for param in model.features.parameters():\r\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA7CLM7rrYQ5"
      },
      "source": [
        "# \r\n",
        "n_inputs = model.classifier[6].in_features\r\n",
        "last_layer = nn.Linear(n_inputs,len(classes))\r\n",
        "model.classifier[6] = last_layer\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy5JxPy9rYT8"
      },
      "source": [
        "# Обучение сети\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=.0001)\r\n",
        "\r\n",
        "epochs =10\r\n",
        "running_loss_history = []\r\n",
        "running_corrects_history = []\r\n",
        "val_running_loss_history = []\r\n",
        "val_running_corrects_history = []\r\n",
        "\r\n",
        "for i in range(epochs):\r\n",
        "  running_loss = 0.0\r\n",
        "  running_corrects= 0.0\r\n",
        "  val_running_loss = 0.0\r\n",
        "  val_running_corrects= 0.0\r\n",
        "  for inputs,labels in training_loader:\r\n",
        "    inputs = inputs.to(device)\r\n",
        "    labels = labels.to(device)\r\n",
        "    \r\n",
        "    outputs = model(inputs)\r\n",
        "    loss = criterion(outputs,labels)\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    _,preds = torch.max(outputs,1)\r\n",
        "   \r\n",
        "    running_loss += loss.item()\r\n",
        "    running_corrects += torch.sum(preds==labels.data)\r\n",
        "\r\n",
        "  else:\r\n",
        "    with torch.no_grad():\r\n",
        "\r\n",
        "      for val_inputs,val_labels in validation_loader:\r\n",
        "        val_inputs = val_inputs.to(device)\r\n",
        "        val_labels = val_labels.to(device)\r\n",
        "        \r\n",
        "        val_outputs = model(val_inputs)\r\n",
        "        val_loss = criterion(val_outputs,val_labels)\r\n",
        "\r\n",
        "        _,val_preds = torch.max(val_outputs,1)\r\n",
        "\r\n",
        "        val_running_loss += val_loss.item()\r\n",
        "        val_running_corrects += torch.sum(val_preds==val_labels.data)\r\n",
        "\r\n",
        "    epoch_loss = running_loss/len(training_loader.dataset)*100\r\n",
        "    epoch_acc =running_corrects.float()/len(training_loader.dataset)*100\r\n",
        "    running_loss_history.append(epoch_loss)\r\n",
        "    running_corrects_history.append(epoch_acc)\r\n",
        "    print('epoch :',i+1)\r\n",
        "    print('training loss : {:.4f},training_acc :{:.4f}'.format(epoch_loss,epoch_acc.item()))\r\n",
        "\r\n",
        "    val_epoch_loss = val_running_loss/len(validation_loader.dataset)*100\r\n",
        "    val_epoch_acc = val_running_corrects.float()/len(validation_loader.dataset)*100\r\n",
        "    val_running_loss_history.append(val_epoch_loss)\r\n",
        "    val_running_corrects_history.append(val_epoch_acc)\r\n",
        "    print('validation loss : {:.4f},val_acc : {:.4f}'.format(val_epoch_loss,val_epoch_acc.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvFawkB9ri0v"
      },
      "source": [
        "plt.plot(running_loss_history)\r\n",
        "plt.plot(val_running_loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFIktIp4ri3c"
      },
      "source": [
        "plt.plot(running_corrects_history)\r\n",
        "plt.plot(val_running_corrects_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU1P7lbpr9oS"
      },
      "source": [
        "dataiter2  = iter(validation_loader)\r\n",
        "images,labels = dataiter2.next()\r\n",
        "images_ = images.to(device)\r\n",
        "labels = labels.to(device)\r\n",
        "\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(25,4))\r\n",
        "output = model(images_)\r\n",
        "_, preds = torch.max(output, 1)\r\n",
        "for idx in np.arange(20):\r\n",
        "  ax = fig.add_subplot(2,10,idx+1,xticks=[],yticks=[])\r\n",
        "  plt.imshow(im_convert(images[idx]))\r\n",
        "  ax.set_title(\"{} ({}) \".format(str(classes[preds[idx].item()]),str(classes[labels[idx].item()])),color = (\"green\" if preds[idx].item()==labels[idx] else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJOBWYZsr9rP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}