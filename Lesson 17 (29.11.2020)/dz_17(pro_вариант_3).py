# -*- coding: utf-8 -*-
"""DZ-17(PRO Вариант 3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mnX3vT7A6m2PAz7DNT8lhNybVWEoJsjJ
"""

# Commented out IPython magic to ensure Python compatibility.
# Статический вывод графики (графики отображаются в той ячейке, в которой используется plt.show())
# %matplotlib inline

import pandas as pd # импортируем библиотеку обработки и анализа данных pandas
import matplotlib.pyplot as plt # Импортируем модуль pyplot библиотеки matplotlib для построения графиков
from tensorflow.keras import utils # Импортируем модуль utils библиотеки tensorflow.keras для получения OHE-представления
import numpy as np # Импортируем библиотеку numpy
from keras.preprocessing.text import Tokenizer, text_to_word_sequence,tokenizer_from_json 

from sklearn.preprocessing import StandardScaler, LabelEncoder # Импортируем библиотеку StandardScaler и LabelEncoder
from sklearn.cluster import KMeans # Импортируем библиотуке KMeans для кластеризации
from sklearn.metrics.cluster import homogeneity_score
from sklearn.manifold import TSNE
import time
import os

from tensorflow.keras.models import Sequential, Model # Полносвязная модель
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Reshape, Conv2DTranspose, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation # Слои для сети
from tensorflow.keras.preprocessing.sequence import pad_sequences # Метод для работы с последовательностями
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.datasets import mnist

# Загрузим данные
(x_train, y_train), (_, _) = mnist.load_data()
X = x_train.astype(np.float32)/255.
X = np.expand_dims(X, -1)
X.shape, X.dtype

# Используем автоэнкодер как модель
def Autoencoder(shape=(28, 28, 1), latent_dim=100): 
    img_input = Input((shape)) 
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(img_input) 
    x = BatchNormalization()(x) 
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 
    x = MaxPooling2D()(x) 

    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x)  
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)  
    x = BatchNormalization()(x) 
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)  
    x = BatchNormalization()(x) 
    x = MaxPooling2D()(x) 
    x = Flatten()(x)
    
    z = Dense(latent_dim, name="latent_space")(x)

    x = Dense(7*7*128)(z)
    x = Reshape((7, 7, 128))(x)
    x = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 
    
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 

    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x) 
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x) 
    x = BatchNormalization()(x) 

    out = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

    model = Model(img_input, out) 
    encoder = Model(img_input, z)

    model.compile(optimizer=Adam(lr=0.0001), loss='mean_squared_error') 

    return model, encoder

# Создадим модель
model, enc = Autoencoder((28, 28, 1))
model.summary()

# Обучим модель
history = model.fit(X, X, epochs=10, batch_size=100, validation_split=0.1)

# Предикт модели
pred = model.predict(X)
pred

plt.figure(figsize=(20,6))
shift = np.random.randint(1000)
for i in range(10):
    plt.subplot(2, 10, 1+i)
    plt.imshow(X[y_train == i][shift].reshape(28,28), cmap='gray')
    plt.subplot(2, 10, 11+i)
    plt.imshow(pred[y_train == i][shift].reshape(28,28), cmap='gray')
plt.show()

z_train = enc.predict(X)
z_train.shape, z_train.std()

# Commented out IPython magic to ensure Python compatibility.
# # Изобразим граффик кластеров
# %%time
# n_clusters = 20               # Зададим количество кластеров
# cost = []                     # Создаем пустой список
# for i in range(1, n_clusters+1): # Пробегаем по списку от 1 до n_clusters
#     kmean = KMeans(i)           # Создаем объект KMeans с i-классами
#     kmean.fit(z_train)   # Проводим кластеризацию z_train
#     cost.append(kmean.inertia_) # Добавляем в cost элемент kmean.inertia_
#     print(i, end=' ')
# print()
# 
# # Отобразим значения списка cost на графике
# plt.plot([i for i in range(1, n_clusters+1)], cost, 'bx-')
# plt.show()

# Кластеризация Метрика homogeneity_score 
km = KMeans(10)
km.fit(z_train)

pred = km.predict(z_train)
print(homogeneity_score(y_train, pred))

# Помения параметры dim=12
model_1, enc_1 = Autoencoder((28, 28, 1), latent_dim=12)

history = model_1.fit(X, X, epochs=20, batch_size=300, validation_split=0.1)

pred = model_1.predict(X)

plt.figure(figsize=(20,6))
shift = np.random.randint(1000)
for i in range(10):
    plt.subplot(2, 10, 1+i)
    plt.imshow(X[y_train == i][shift].reshape(28,28), cmap='gray')
    plt.subplot(2, 10, 11+i)
    plt.imshow(pred[y_train == i][shift].reshape(28,28), cmap='gray')
plt.show()

z_train3 = enc_1.predict(X)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# n_clusters = 16               # Зададим количество кластеров
# cost = []                     # Создаем пустой список
# for i in range(1, n_clusters+1): # Пробегаем по списку от 1 до n_clusters
#     kmean = KMeans(i)           # Создаем объект KMeans с i-классами
#     kmean.fit(z_train3)   # Проводим кластеризацию z_train
#     cost.append(kmean.inertia_) # Добавляем в cost элемент kmean.inertia_
#     print(i, end=' ')
# print()
# 
# # Отобразим значения списка cost на графике
# plt.plot([i for i in range(1, n_clusters+1)], cost, 'bx-')
# plt.show()

# Кластеризация Метрика homogeneity_score 
km = KMeans(10)
km.fit(z_train3)

pred = km.predict(z_train3)
print(homogeneity_score(y_train, pred))