# -*- coding: utf-8 -*-
"""DZ - Light - 11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WBcK7I4-uGN7y4hPtDkvwqDH9BIAVBhg
"""

# Commented out IPython magic to ensure Python compatibility.
# Загружаем модули
from tensorflow.keras.datasets import mnist, cifar10
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda
from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D, concatenate, Embedding, multiply
from tensorflow.keras.layers import LeakyReLU, UpSampling2D, Conv2D 
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
from keras import backend as K
from tensorflow.keras.layers import LeakyReLU
try:
#   %tensorflow_version 2.x
except Exception:
  pass
  
import matplotlib.pyplot as plt
import sys
import numpy as np

# Подключаем Гугл диск
from google.colab import drive
drive.mount('/content/drive')

!mkdir images

# MNIST
batch_size = 128
(X_train, _), (_, _) = mnist.load_data()
X_train = X_train / 127.5 - 1.
X_train = np.expand_dims(X_train, axis=3)

# MNIST Size
img_rows_mnist = 28
img_cols_mnist = 28
channels_mnist = 1
img_shape_mnist = (img_rows_mnist, img_cols_mnist, channels_mnist)
latent_dim = 100
batch_size_mnist = 128

def get_optimizer():
  return Adam(learning_rate = 0.0002, beta_1 = 0.5)

def create_generator_mnist():
  generator = Sequential()

  generator.add(Dense(256, input_dim = latent_dim))
  generator.add(LeakyReLU(alpha = 0.2))
  generator.add(BatchNormalization(momentum = 0.8))

  generator.add(Dense(512))
  generator.add(LeakyReLU(alpha = 0.2))
  generator.add(BatchNormalization(momentum = 0.8))

  generator.add(Dense(1024))
  generator.add(LeakyReLU(alpha = 0.2))
  generator.add(BatchNormalization(momentum = 0.8))

  generator.add(Dense(np.prod(img_shape_mnist), activation='tanh'))
  generator.add(Reshape(img_shape_mnist))

  noise = Input(shape=(latent_dim,))
  img = generator(noise)
  generator = Model(noise, img)
  return generator

generator_mnist = create_generator_mnist()
generator_mnist.summary()

def create_discriminator_mnist():
  dis = Sequential()

  dis.add(Conv2D(4, (3, 3), padding = 'same', input_shape = img_shape_mnist))
  dis.add(LeakyReLU(alpha = 0.2))
  dis.add(Flatten())
  dis.add(Dense(512))
  dis.add(LeakyReLU(alpha = 0.2))
  dis.add(Dense(1, activation='sigmoid'))

  img = Input(shape=img_shape_mnist)
  validity = dis(img)
  discriminator_mnist = Model(img, validity)
  discriminator_mnist.compile(loss = 'binary_crossentropy', optimizer = get_optimizer(), metrics = ['accuracy'])
  discriminator_mnist.trainable = False
  return discriminator_mnist

discriminator_mnist = create_discriminator_mnist()
discriminator_mnist.summary()

input1 = np.random.random(size=(1, 100))
plt.imshow(input1, cmap='gray')
plt.axis('off')
plt.show()
predict_gen = generator_mnist.predict(input1)
plt.imshow(predict_gen[0].reshape(28, 28), cmap='gray')
plt.show()

# Функция печати изображений
def get_images(epoch, gen, noise):
  r,c = 5, 5
  gen_img = generator_mnist.predict(noise)
  fig, axs = plt.subplots(r, c)
  cnt = 0
  for i in range(r):
    for j in range(c):
      axs[i, j].imshow(gen_img[cnt, :, :, 0], cmap='gray')
      axs[i, j].axis('off')
      cnt += 1
  plt.show()
  fig.savefig('images/%d.png' % epoch)
  plt.close()

# Функция обучения 
def train(generator, discriminator, epochs=30000, batch_size=128): 

  valid = np.ones((batch_size, 1))
  fake = np.zeros((batch_size, 1))
  r, c = 5, 5
  z = Input(shape=(latent_dim,))
  img = generator_mnist(z)
  validity = discriminator_mnist(img)

  combined = Model(z, validity)
  combined.compile(loss = 'binary_crossentropy', optimizer = get_optimizer())

  sample_interval = 100

  d_loss_list = []
  g_loss_list = []
  d_acc_list = []

  CONST_NOISE = np.random.normal(0, 1, (r * c, latent_dim))
  # Train Discriminator
  for epoch in range(epochs):

    idx = np.random.choice(X_train.shape[0], batch_size, replace = False)
    imgs = X_train[idx]
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    gen_imgs = generator_mnist.predict(noise)

    d_loss_real = discriminator.train_on_batch(imgs, valid) 
    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # Train Generator
    g_loss = combined.train_on_batch(noise, valid)
    print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))
    d_loss_list.append(d_loss[0])
    g_loss_list.append(g_loss)
    d_acc_list.append(100 * d_loss[1])

    if ((epoch % sample_interval == 0) | (epoch == epoch-1)):
      get_images(epoch, generator_mnist, CONST_NOISE)

  plt.plot(d_loss_list, label="Ошибка дискриминатора")
  plt.plot(g_loss_list, label="Ошибка генератора")
  plt.legend()
  plt.show()
  plt.plot(d_acc_list, label="Точность распознавания дискриминатора")
  plt.legend()
  plt.show()

train(generator_mnist, discriminator_mnist, 3000)