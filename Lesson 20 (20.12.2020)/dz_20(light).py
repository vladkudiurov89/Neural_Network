# -*- coding: utf-8 -*-
"""DZ-20(Light).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oBKYsmYUr6GxAOAoSka-O-VWR7N7JBca
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

!pip install git+https://github.com/librosa/librosa

!pip install jiwer

!pip install SpeechRecognition

import speech_recognition as sR
from jiwer import wer # модуль метрики качества в распознавании речи
import numpy as np # библиотека для работы с массивами данных
from matplotlib import pyplot as plt # интерфейс для построения графиков простых функций
from IPython.display import HTML, Audio # загружаем модуль чтобы обратиться к HTML для записи аудио с микрофона в ноутбуке
from google.colab import files # модуль для загрузки файлов
from google.colab.output import eval_js
from base64 import b64decode # модуль для кодировки/раскодировки аудиозаписи(64-разрядный код)
from scipy.io.wavfile import read as wav_read # для чтения WAV формата
import io
import scipy # воспользуемся модулями библиотеки для работы со звуковой дорожкой
import librosa # для параметризации аудио
import os # модуль для работы с операционной системой(воспользуемся методами работы с каталогами)
from sklearn.model_selection import train_test_split # модуль для разбивки данных на обучающую и тестовую выборки
from keras.utils import to_categorical # загружаем утилиты кераса для one hot кодировки
from tqdm import tqdm # быстрый, удобный progress bar для Python
from keras.optimizers import Adam, RMSprop, Adadelta # загружаем алгоритмы обучения/оптимизации
from keras.models import Sequential # последовательная модель нейросети кераса
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization # загружаем необходимые слои для нейросети
import IPython.display as ipd #Для проигрывания аудио

# подключаем гугл драйв диск для загрузки файлов
from google.colab import drive 
drive.mount('/content/drive')

!rm -R /content/AudioComands

!unzip -q '/content/drive/My Drive/datasets/voice/comands-2020.zip' -d '/content/AudioComands'

!ls '/content/AudioComands/comands'

!unzip -q '/content/drive/My Drive/datasets/voice/test_speech-2020.zip' -d '/content/AudioTest'

!ls '/content/AudioTest'

# Указываем путь
DATA_PATH = '/content/AudioComands/comands'
DATA_PATH_TEST = '/content/AudioTest/test_speech'

# Укажем параметры
sample_rate = 22050
feature_dim_1 = 20
feature_dim_2 = int(0.5 * sample_rate)
step_mfcc = int(0.02 * sample_rate)
channel = 1
classes = ['conditioner', 'light', 'tv']
n_classes = len(classes) + 1

# Функцию извлечения лейблов классов и их обработки
def get_labels(path=DATA_PATH): 
  labels = sorted(os.listdir(path)) # запишем лейблы классов по названию папок с данными - ['кондиционер', 'телевизор', 'свет', 'фон']  
  label_indices = np.arange(0, len(labels)) # запишем лейблы в виде индексов - [0, 1, 2, 3]
  return labels, label_indices, to_categorical(label_indices) # функция вернёт лейблы, индексы лейблов и их one-hot представление[0, 1, 0, 0]

labels, _, _ = get_labels(path=DATA_PATH)
print(labels)

# Функция для получения обучающей и проверочной выборки
def get_train_test_sets(path, labels, train_size=0.8, random_state=42):
  class_filenames = []
  for i in labels:
    class_filenames.append(os.listdir(os.path.join(path, i)))

  train_fn = [] 
  test_fn = []

  for i in range(len(class_filenames)):
    class_len = len(class_filenames[i])
    train_num = np.random.choice(class_len, int(class_len * train_size), replace=False) 
    test_num = np.array(list(set(range(class_len)) - set(train_num)))
  
    train_fn.append(np.array(class_filenames[i])[train_num])
    test_fn.append(np.array(class_filenames[i])[test_num])

    print(f'Class {labels[i]}, train_samples={len(train_fn[-1])}, test_samples={len(test_fn[-1])}')

  return train_fn, test_fn

# Функция для аудио параметризации
def get_audio_parrametrs(path, labels, fn):
  audio_data = []
  class_data = []

  for i, label in enumerate(labels):
    for wavefile in tqdm(fn[i], f'EXTRACT CLASS AUDIO - {label}'):
      file_path = os.path.join(path, label, wavefile)
      y, sr = librosa.load(file_path)
      audio_data.append(y)
      class_data.append(i) 
  return audio_data, class_data

train_fn, test_fn = get_train_test_sets(DATA_PATH, labels, train_size=0.8, random_state=42)

audio_train, class_train = get_audio_parrametrs(DATA_PATH, labels, train_fn)
audio_test, class_test = get_audio_parrametrs(DATA_PATH, labels, test_fn)

print(f'Train selection')
for i in range(len(labels)):
  c_ind = np.where(np.array(class_train) == i)[0]
  c_len = 0
  for ind in c_ind:
    c_len += len(audio_train[ind])
  print(f'Class {labels[i]} total audio lenght = {c_len}')

print(f'\nTest Selection')
for i in range(len(labels)):
  c_ind = np.where(np.array(class_test) == i)[0]
  c_len = 0
  for ind in c_ind:
    c_len += len(audio_test[ind])
  print(f'Class {labels[i]} total audio lenght = {c_len}')

file_path = os.path.join(DATA_PATH, '1_cond', train_fn[0][0])
y, sr = librosa.load(file_path, sr=29 * 1000)
ipd.Audio(data=y, rate=22050)

def get_audio_parrametrs_ext(path, labels, fn, rate_diapasone):
  a_data = []
  c_data = []

  for i, label in enumerate(labels):
    for wfile in tqdm(fn[i], f'EXTRACT CLASS AUDIO - {label}'):
      file_path = os.path.join(path, label, wfile)
      if label != labels[-1]:
        for rate in rate_diapasone:
          y, sr = librosa.load(file_path, sr = rate * 1000)
          a_data.append(y)
          c_data.append(i)
      else:
        y, sr = librosa.load(file_path)
        a_data.append(y)
        c_data.append(i)

  return a_data, c_data

audio_train, class_train = get_audio_parrametrs_ext(DATA_PATH, labels, train_fn, range(15, 30))
audio_test, class_test = get_audio_parrametrs_ext(DATA_PATH, labels, test_fn, range(15, 30))

audio_train = np.array(audio_train)
audio_test = np.array(audio_test)
class_train = np.array(class_train)
class_test = np.array(class_test)

# np.save('/content/drive/My Drive/lesson20/audio_train.npy', audio_train)
# np.save('/content/drive/My Drive/lesson20/audio_test.npy', audio_test)
# np.save('/content/drive/My Drive/lesson20/class_train.npy', class_train)
# np.save('/content/drive/My Drive/lesson20/class_test.npy', class_test)

audio_train = np.load('/content/drive/My Drive/lesson20/audio_train.npy', allow_pickle=True)
audio_test = np.load('/content/drive/My Drive/lesson20/audio_test.npy', allow_pickle=True)
class_train = np.load('/content/drive/My Drive/lesson20/class_train.npy', allow_pickle=True)
class_test = np.load('/content/drive/My Drive/lesson20/class_test.npy', allow_pickle=True)

ipd.Audio(audio_test[95], rate=22050)

def wav2mfcc(y, length=11025, step=2205):
  out_mfcc = []
  out_audio = []

  while (len(y) >= length):
    section = y[:length]
    section = np.array(section)
    out_mfcc.append(librosa.feature.mfcc(section, sr=22050))
    out_audio.append(section)
    y = y[step:]

  out_mfcc = np.array(out_mfcc)
  out_audio = np.array(out_audio)

  return out_mfcc, out_audio

def getXYData(audio_data, class_data, length, step_mfcc):

  x_data = []
  y_data = []

  for i in range(len(audio_data)):
    mfccs, _ = wav2mfcc(audio_data[i], length, step_mfcc)
    if mfccs.shape[0] != 0:
      for mfcc in mfccs:
        x_data.append(mfcc)
        y_data.append(to_categorical(class_data[i], n_classes))

  return np.array(x_data), np.array(y_data)

xTrain, yTrain = getXYData(audio_train, class_train, feature_dim_2, step_mfcc)
xTest, yTest = getXYData(audio_test, class_test, feature_dim_2, step_mfcc)

print(xTrain.shape, yTrain.shape)
print(xTest.shape, yTest.shape)

# np.save('/content/drive/My Drive/lesson20/xTrain.npy', xTrain)
# np.save('/content/drive/My Drive/lesson20/xTest.npy', xTest)
# np.save('/content/drive/My Drive/lesson20/yTrain.npy', yTrain)
# np.save('/content/drive/My Drive/lesson20/yTest.npy', yTest)

xTrain = np.load('/content/drive/My Drive/lesson20/xTrain.npy', allow_pickle=True)
xTest =  np.load('/content/drive/My Drive/lesson20/xTest.npy', allow_pickle=True)
yTrain = np.load('/content/drive/My Drive/lesson20/yTrain.npy', allow_pickle=True)
yTest =  np.load('/content/drive/My Drive/lesson20/yTest.npy', allow_pickle=True)

xTrain = xTrain[..., None]
xTest = xTest[..., None]

print(xTrain.shape, yTrain.shape)
print(xTest.shape, yTest.shape)

print(np.min(xTrain), np.max(xTrain))
print(np.min(xTest), np.max(xTest))

from sklearn.preprocessing import StandardScaler
from keras.layers import Input
from keras.models import Model

shape = xTrain.shape
xScaler = StandardScaler()
xScaler.fit(xTrain.reshape(-1, shape[1]))
xTrainS = xScaler.transform(xTrain.reshape(-1, shape[1])).reshape(shape)

shape = xTest.shape
xTestS = xScaler.transform(xTest.reshape(-1, shape[1])).reshape(shape)

print(xTrainS.shape, xTestS.shape)
print(np.min(xTrainS), np.max(xTrainS))
print(np.min(xTestS), np.max(xTestS))

def create_model():

  data_input = Input(shape=(xTrain[0].shape))
  x = Conv2D(8, (3,3), activation='relu')(data_input)
  x = MaxPooling2D()(x)
  x = BatchNormalization()(x)
  x = Flatten()(x)
  x = Dense(128, activation='relu')(x)
  x = Dropout(0.25)(x)
  x = BatchNormalization()(x)
  data_output = Dense(n_classes, activation='softmax')(x)

  model = Model(data_input, data_output)

  model.compile(loss='categorical_crossentropy',
                optimizer=Adam(1e-3),
                metrics=['accuracy'])
  
  return model

model = create_model()
model.summary()

history = model.fit(xTrainS, yTrain,
                    batch_size=32,
                    epochs=20,
                    validation_data=[xTestS, yTest])

plt.plot(history.history['accuracy'], label='Train accuracy')
plt.plot(history.history['val_accuracy'], label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

!unzip -q '/content/drive/My Drive/datasets/voice/test_speech-2020.zip' -d '/content/AudioTest'

!ls '/content/AudioTest/test_speech'

wavfiles =  DATA_PATH_TEST + '/text_with_cond3.wav'
y,sr = librosa.load(wavfiles)
print('Оригинальный файл:')
ipd.Audio(data=y, rate = sr)

mfcc_full, audio_full = wav2mfcc (y, length=feature_dim_2, step = step_mfcc) 

shape = mfcc_full.shape
print(shape)
mfcc_full = xScaler.transform(mfcc_full.reshape(-1, shape[1])).reshape(shape)

mfcc_full = mfcc_full.reshape(-1, 20, 22, 1) 
g_pred = model.predict(mfcc_full) 
pred = np.array([np.argmax(i) for i in g_pred])

print(g_pred.shape)
print(pred.shape)
print(pred[:800])

idxs = np.where(pred == 0)[0]
print(idxs)

curr = []

curr_idx = int(idxs[0])
summ, length = 0, 0

for i in range(len(idxs)):
  summ += g_pred[idxs[i]][0]
  length += 1
  if i == len(idxs)-1:
    if length >= 2 and summ/length >= 0.95:
      curr.append([curr_idx, length, summ/length])
    break
  if idxs[i+1]-idxs[i] > 1:
    if (length >= 8 and summ/length >= 0.95):
      curr.append([curr_idx, length, summ/length])
    curr_idx = int(idxs[i+1])
    summ, length = 0, 0

print(curr)

np_Audio = []
curr_audio = []

for elem in curr:
  curr_audio = audio_full[elem[0]]
  if elem[0] != 0:
    curr_audio = np.hstack((audio_full[elem[0]-1], audio_full[elem[0]][-step_mfcc:]))
  for j in range(1,elem[1]):
    if elem[0]+j == len(audio_full):
      break
    curr_audio = np.hstack((curr_audio, audio_full[elem[0]+j][-step_mfcc:]))
  curr_audio = np.array(curr_audio)
  np_Audio.append([curr_audio, elem[2]])

if (len(np_Audio)==0):
    print('Команда не распознанана!!!')
for elem in np_Audio: 
    print ('Распознана команда: "', classes[0], '" (вероятность - %.2f' % (elem[1]*100), '%)')
    ipd.display(ipd.Audio (elem[0], rate = 22050))

def getPredict(y, model, min_count=2, rate=0.9, hole=1, read=False):

  # Если аудио не загружено, то читаем его либрозой
  if read:
    y, sr = librosa.load(y)

  # Получаем фичи
  mfcc_full, audio_full = wav2mfcc(y, length=feature_dim_2, step=step_mfcc)

  # Если фич нет (к примеру, аудио слишком короткое) выходим с пустыми значениями
  if len(mfcc_full) == 0:
    return [], [], []

  # Нормируем данные
  shape = mfcc_full.shape
  mfcc_full = xScaler.transform(mfcc_full.reshape(-1, shape[1])).reshape(shape)

  # Решейпим под предикт и предиктим
  mfcc_full = mfcc_full.reshape(-1, shape[1], shape[2], 1) 
  g_pred = model.predict(mfcc_full) 
  pred = np.array([np.argmax(i) for i in g_pred])

  out = []

  # Получаем промежутки аудио определенного класса, если они есть
  for idx_class in range(n_classes-1):
    idxs = np.where(pred == idx_class)[0]

    if len(idxs) == 0:
      continue

    curr = []
    curr_idx = int(idxs[0])
    summ, length = 0, 0

    for i in range(len(idxs)):
      summ += g_pred[idxs[i]][idx_class]
      length += 1

      if i == len(idxs)-1:
        if length >= min_count and summ/length >= rate:
          curr.append([curr_idx, length, summ/length])
        break

      if idxs[i+1]-idxs[i] > hole:
        if length >= min_count and summ/length >= rate:
          curr.append([curr_idx, length, summ/length])
        curr_idx = int(idxs[i+1])
        summ, length = 0, 0
    
    # Собираем класс его выделенную на аудио область и предикт в один список
    curr_audio = []

    for elem in curr:
      curr_audio = audio_full[elem[0]]

      for j in range(1, elem[1]):
        if elem[0]+j == len(audio_full):
          break
        
        curr_audio = np.hstack((curr_audio, audio_full[elem[0]+j][-step_mfcc:]))
      curr_audio = np.array(curr_audio)
      out.append([curr_audio, idx_class, elem[2]])

  return out, pred, g_pred

# Напишем функцию для проверки определения комманды на всех тестовых аудиозаписях

def getResultStat(model):

  prediction_list = [] # Сюда будем записывать результаты предсказания класса

  for audio in tqdm(audio_test, f'Test audio'):
    out, pred, _ = getPredict(audio, model=model, min_count = 2, rate = 0.9, hole = 1)
    if (len(out)==0):
      prediction_list.append(3) # Если класс не определился - значит шум
    else:
      sorrted_out = sorted(out, key=lambda x: x[2], reverse=True) # Выберем максимальное значение и запишем его как предсказание
      prediction_list.append(sorrted_out[0][1])

  print()
  prediction_list = np.array(prediction_list)

  # Посчитаем для каждого класса точность как процент от правильно пределенных записей класса деленный на их количество
  for i in range(n_classes):

    class_ind = np.where(class_test == i)[0]
    class_acc = 0
    for ind in class_ind:

      if int(prediction_list[ind]) == int(class_test[ind]):
        class_acc += 1
    if i != n_classes-1:
      print(f'Class {classes[i]} accuracy {round(class_acc*100/len(class_ind), 2)} %')
    else:
      print(f'Class noise accuracy {round(class_acc*100/len(class_ind), 2)} %')

wavfiles =  DATA_PATH_TEST + '/text_with_svet'+str(12)+'.wav' # Получаем имя очередного файла
out, pred, _ = getPredict(wavfiles, model=model, min_count=2, rate=0.9, hole=1, read=True)  # Вызываем predict для очередного файла  
if (len(out)==0): # Если длинна массива равна 0, то команда не распознана
  print('Команда не распознанана!!!')
for elem in out: # Пробегам по всем элементам массива out
  print ('Распознана команда: "', classes[elem[1]], '" (вероятность - %.2f' % (elem[2]*100), '%)') # Выводим название
  ipd.display(ipd.Audio (elem[0], rate = 22050))

getResultStat(model)

model = create_model()
history = model.fit(xTrainS, yTrain,
                    batch_size=64,
                    epochs=100,
                    validation_data=[xTestS, yTest])

plt.plot(history.history['accuracy'], label='Train accuracy')
plt.plot(history.history['val_accuracy'], label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

getResultStat(model)

model = create_model()
history = model.fit(xTrainS, yTrain,
                    batch_size=128,
                    epochs=100,
                    validation_data=[xTestS, yTest])

plt.plot(history.history['accuracy'], label='Train accuracy')
plt.plot(history.history['val_accuracy'], label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

getResultStat(model)

model = create_model()
history = model.fit(xTrainS, yTrain,
                    batch_size=256,
                    epochs=100,
                    validation_data=[xTestS, yTest])

plt.plot(history.history['accuracy'], label='Train accuracy')
plt.plot(history.history['val_accuracy'], label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

getResultStat(model)

# Уберем предупреждения, так как при считывании либрозой .m4a файлов они возникают и засоряют вывод
import warnings
warnings.filterwarnings('ignore')

def getResult(model, audio, min_count=2, rate=0.9, hole=1):

  out, pred, _ = getPredict(audio, model=model, min_count=2, rate=0.9, hole=1) # Получаем предсказанный множество предскзанных учатсков
  if (len(out)==0): # Если таких участков записи нет, то определяем запись как "шум"
    class_number = 3 
  else: # Если участки есть, то записываем класс с наибольшей вероятностью
    sorrted_out = sorted(out, key=lambda x: x[2], reverse=True)
    class_number = sorrted_out[0][1]

  if class_number != n_classes-1:
    string = f'{classes[class_number]}'
  else:
    string = f'noise'
  
  return string

additional_files = '/content/drive/My Drive/lesson20/test_audio/'

for filename in sorted(os.listdir(additional_files)):
  y, sr = librosa.load(os.path.join(additional_files, filename), sr=22050, mono=True)
  y, index = librosa.effects.trim(y)
  ipd.display(ipd.Audio(y, rate=22050))
  string = getResult(model, y, min_count=3, rate=0.95, hole=1)

  print(f'Filename "{filename}" classified as "{string}"', end='\n\n\n')

add_audio_train = []
add_class_train = []
add_audio_test = []
add_class_test = []


for filename in sorted(os.listdir(additional_files)):
  y, sr = librosa.load(os.path.join(additional_files, filename), sr=22050, mono=True)
  if '1' in filename or '5' in filename:
    add_audio_test.append(y)
    if 'Cond' in filename:
      add_class_test.append(0)
    if 'Light' in filename:
      add_class_test.append(1)
    else:
      add_class_test.append(2)
  else:
    add_audio_train.append(y)
    if 'Cond' in filename:
      add_class_train.append(0)
    if 'Light' in filename:
      add_class_train.append(1)
    else:
      add_class_train.append(2)


add_audio_train = np.array(add_audio_train)
add_class_train = np.array(add_class_train)

add_audio_test = np.array(add_audio_test)
add_class_test = np.array(add_class_test)

xAddTrian, yAddTrain = getXYData(add_audio_train, add_class_train, feature_dim_2, step_mfcc)
xAddTest, yAddTest = getXYData(add_audio_test, add_class_test, feature_dim_2, step_mfcc)

xAddTrian = xAddTrian[..., None]
xAddTest = xAddTest[..., None]

shape = xAddTrian.shape
xAddTrian = xScaler.transform(xAddTrian.reshape(-1, shape[1])).reshape(shape)

shape = xAddTest.shape
xAddTest = xScaler.transform(xAddTest.reshape(-1, shape[1])).reshape(shape)

print(xAddTrian.shape, xAddTest.shape)

xTrainS = np.concatenate([xTrainS, xAddTrian], axis=0)
yTrain = np.concatenate([yTrain, yAddTrain], axis=0)

xTestS = np.concatenate([xTestS, xAddTest], axis=0)
yTest = np.concatenate([yTest, yAddTest], axis=0)

print(xTrainS.shape, yTrain.shape)
print(xTestS.shape, yTest.shape)

model = create_model()
history = model.fit(xTrainS, yTrain,
                    batch_size=256,
                    epochs=100,
                    validation_data=[xTestS, yTest])

plt.plot(history.history['accuracy'], label='Train accuracy')
plt.plot(history.history['val_accuracy'], label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

getResultStat(model)

for i, y in enumerate(add_audio_test):
  string = getResult(model, y, min_count=5, rate=0.9, hole=1)
  ipd.display(ipd.Audio(y, rate=22050))
  print(f'Class "{classes[add_class_test[i]]}" classified as "{string}"', end='\n\n\n')

