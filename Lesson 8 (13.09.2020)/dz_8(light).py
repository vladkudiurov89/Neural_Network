# -*- coding: utf-8 -*-
"""DZ - 8(Light)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k7ltzdQwmInteC6quMX_e0HoroXw_XgC
"""

# Commented out IPython magic to ensure Python compatibility.
#Подключаем библиотеки
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import concatenate, Input, Dense, Dropout, BatchNormalization, Flatten, Conv1D, Conv2D, LSTM
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

from google.colab import files
from tensorflow.keras import utils
import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import time
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

import zipfile #Для разорхивации базы

#Разорхивируем архив с базой
z = zipfile.ZipFile('/content/drive/My Drive/datasets/voice/genres.zip', 'r')
z.extractall()
genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()

#Проверяем выгруженные папки
!ls genres 
#И одну из папок
!ls genres/blues

#Функция параметризации аудир
def get_features(y, sr):
  #Получаем различные параметры аудио
  chroma_stft = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))
  rmse = np.mean(librosa.feature.rmse(y=y))
  spec_cent = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
  spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))
  rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))
  zcr = np.mean(librosa.feature.zero_crossing_rate(y))
  mfcc = librosa.feature.mfcc(y=y, sr=sr)
  
  #Добавляем все параметры в один список
  out = []
  out.append(chroma_stft)
  out.append(rmse)
  out.append(spec_cent)
  out.append(spec_bw)
  out.append(rolloff)
  out.append(zcr)
  
  #По одному добавляем все Мел коэффициенты
  for e in mfcc:
    out.append(np.mean(e))
  
  #Возвращаем получившийся список
  return out

#Формируем обучающую выборку
#Создаём пустые листы
X_train = []
Y_train = []

#Запоминаем время старта формирования выборки
curr_time = time.time()

#Проходим по всем жарнам
for i in range(len(genres)):
  g = genres[i] 
  for filename in os.listdir(f'./genres/{g}'):
    songname = f'./genres/{g}/{filename}'
    y, sr = librosa.load(songname, mono=True, duration=30)
    out = get_features(y, sr)

    X_train.append(out)
    Y_train.append(utils.to_categorical(i, len(genres)))

  print("Жанр ", g, " готов -> ", round(time.time() - curr_time), "c", sep="")
  curr_time = time.time()

#Превращаем обучающую выборку на numpy массивы
X_train = np.array(X_train)
Y_train = np.array(Y_train)

print(X_train.shape)
print(Y_train.shape)
print(y_train_class.shape)

#Создаём backup обучающей выборки
X_train_backup = X_train.copy()
Y_train_backup = Y_train.copy()
columns = ['dense', 'neuron', 'drop', 'BN', 'activation', 'batch_size', 'acc', 'val_acc']

y_train_class = np.argmax(Y_train, axis=1)
print(y_train_class)

#Выводим размеры обучающей выборки
print(X_train.shape)
print(Y_train.shape)
print(y_train_class.shape)

#Создаём scaler экземпляр класса StandardScaler() для нормировки данных
scaler = StandardScaler()
#Номируем X_train
X_train = scaler.fit_transform(X_train)

#Разделяем выборку на обучающую и проверочную
#Для проверочной используем 10% примеров
#Так как база маленькая
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train_class, test_size=0.1, shuffle=True)

#Выводим размеры обучающей и проверочной выборки для проверки

print(X_train.shape)
print(X_test.shape)

print(y_train.shape)
print(y_test.shape)

data = []

indexes = range(0,26)

#Создаём полносвязанную сеть 
model = Sequential()
model.add(Dense(256, activation='elu', input_shape=(len(indexes),)))
model.add(Dense(128, activation='elu'))
model.add(Dense(64, activation='elu'))
model.add(Dense(32, activation='elu'))
#В конце количество нейронов равно количеству классов и softmax
model.add(Dense(len(genres), activation='softmax'))

#Компилируем сеть
model.compile(optimizer=RMSprop(lr=1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#Обучаем
history = model.fit(X_train[:, indexes],
                    y_train,
                    epochs=150,
                    batch_size=20,
                    validation_data=(X_test[:, indexes], y_test))

data +=[[4, '256-128-64-32', '-', '-', 'elu', 20, history.history["accuracy"][-1], history.history["val_accuracy"][-1]]]
#Выводим график точности распознавания на обучающей и проверочной выборках
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.show()

#Создаём полносвязанную сеть 
model = Sequential()
model.add(Dense(10, activation='elu', input_shape=(len(indexes),)))
model.add(Dense(len(genres), activation='softmax'))

#Компилируем сеть
model.compile(optimizer=RMSprop(lr=1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#Обучаем
history = model.fit(X_train[:, indexes],
                    y_train,
                    epochs=150,
                    batch_size=20,
                    validation_data=(X_test[:, indexes], y_test))

data +=[[1, '10', '-', '-', 'elu', 20, history.history["accuracy"][-1], history.history["val_accuracy"][-1]]]
#Выводим график точности распознавания на обучающей и проверочной выборках
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.show()

#Создаём полносвязанную сеть 
model = Sequential()
model.add(Dense(1024, activation='elu', input_shape=(len(indexes),)))
model.add(Dense(512, activation='elu'))
model.add(Dense(256, activation='elu'))
model.add(Dense(128, activation='elu'))
model.add(Dense(128, activation='elu'))
model.add(Dense(128, activation='elu'))
#В конце количество нейронов равно количеству классов и softmax
model.add(Dense(len(genres), activation='softmax'))

#Компилируем сеть
model.compile(optimizer=RMSprop(lr=1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#Обучаем
history = model.fit(X_train[:, indexes],
                    y_train,
                    epochs=150,
                    batch_size=20,
                    validation_data=(X_test[:, indexes], y_test))

data +=[[6, '1024-512-256-128-128-128', '-', '-', 'elu', 20, history.history["accuracy"][-1], history.history["val_accuracy"][-1]]]
#Выводим график точности распознавания на обучающей и проверочной выборках
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.show()

#Создаём полносвязанную сеть 
model = Sequential()
model.add(Dense(256, activation='elu', input_shape=(len(indexes),)))
model.add(Dropout(0.2))
model.add(Dense(128, activation='elu'))
model.add(Dense(64, activation='elu'))
model.add(Dense(32, activation='elu'))
#В конце количество нейронов равно количеству классов и softmax
model.add(Dense(len(genres), activation='softmax'))

#Компилируем сеть
model.compile(optimizer=RMSprop(lr=1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#Обучаем
history = model.fit(X_train[:, indexes],
                    y_train,
                    epochs=150,
                    batch_size=20,
                    validation_data=(X_test[:, indexes], y_test))

data +=[[4, '256-128-64-32', '+', '-', 'elu', 20, history.history["accuracy"][-1], history.history["val_accuracy"][-1]]]
#Выводим график точности распознавания на обучающей и проверочной выборках
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.show()

#Создаём полносвязанную сеть 
model = Sequential()
model.add(Dense(256, activation='elu', input_shape=(len(indexes),)))
model.add(BatchNormalization())
model.add(Dense(128, activation='elu'))
model.add(Dense(64, activation='elu'))
model.add(Dense(32, activation='elu'))
#В конце количество нейронов равно количеству классов и softmax
model.add(Dense(len(genres), activation='softmax'))

#Компилируем сеть
model.compile(optimizer=RMSprop(lr=1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#Обучаем
history = model.fit(X_train[:, indexes],
                    y_train,
                    epochs=150,
                    batch_size=20,
                    validation_data=(X_test[:, indexes], y_test))

data +=[[4, '256-128-64-32', '-', '+', 'elu', 20, history.history["accuracy"][-1], history.history["val_accuracy"][-1]]]
#Выводим график точности распознавания на обучающей и проверочной выборках
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.show()

activ = ['relu', 'selu', 'linear']
for i in activ:
        
    #Создаём полносвязанную сеть 
    model = Sequential()
    model.add(Dense(256, activation=i, input_shape=(len(indexes),)))
    model.add(Dense(128, activation=i))
    model.add(Dense(64, activation=i))
    model.add(Dense(32, activation=i))
    #В конце количество нейронов равно количеству классов и softmax
    model.add(Dense(len(genres), activation='softmax'))

    #Компилируем сеть
    model.compile(optimizer=RMSprop(lr=1e-4),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

    #Обучаем
    history = model.fit(X_train[:, indexes],
                        y_train,
                        epochs=150,
                        batch_size=20,
                        validation_data=(X_test[:, indexes], y_test))

    data +=[[4, '256-128-64-32', '-', '-', i, 20, history.history["accuracy"][-1], history.history["val_accuracy"][-1]]]
    #Выводим график точности распознавания на обучающей и проверочной выборках
    plt.plot(history.history["accuracy"])
    plt.plot(history.history["val_accuracy"])
    plt.show()

batch_s = [20, 64, 256, 1024]
for i in batch_s:
        
    #Создаём полносвязанную сеть 
    model = Sequential()
    model.add(Dense(256, activation='elu', input_shape=(len(indexes),)))
    model.add(Dense(128, activation='elu'))
    model.add(Dense(64, activation='elu'))
    model.add(Dense(32, activation='elu'))
    #В конце количество нейронов равно количеству классов и softmax
    model.add(Dense(len(genres), activation='softmax'))

    #Компилируем сеть
    model.compile(optimizer=RMSprop(lr=1e-4),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

    #Обучаем
    history = model.fit(X_train[:, indexes],
                        y_train,
                        epochs=150,
                        batch_size=i,
                        validation_data=(X_test[:, indexes], y_test))

    data +=[[4, '256-128-64-32', '-', '-', 'elu', i, history.history["accuracy"][-1], history.history["val_accuracy"][-1]]]
    #Выводим график точности распознавания на обучающей и проверочной выборках
    plt.plot(history.history["accuracy"])
    plt.plot(history.history["val_accuracy"])
    plt.show()

import pandas as pd
df = pd.DataFrame(data, columns= columns)
df

