# -*- coding: utf-8 -*-
"""DZ-5(LIGHT).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15MojFUYWDipwulHY1F9HCIy0JRmBVFey
"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras import utils
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, Activation
from tensorflow.keras.layers import Conv1D, SpatialDropout1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D
from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Bidirectional
from tensorflow.keras.optimizers import Adam, RMSprop
from keras.utils import plot_model

import sklearn
from sklearn.metrics import precision_recall_fscore_support as score
from keras.preprocessing.text import Tokenizer, text_to_word_sequence  
from sklearn.model_selection import train_test_split  

from google.colab import files
import numpy as np               
import pandas as pd              
import matplotlib.pyplot as plt  
import os                        
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

!rm -R /content/texts # проверяем наличие папки texts; если есть, удаляем ее

# указываем путь к базе в Google Drive, создаем папку texts и извлекаем туда базу
!unzip -q '/content/drive/My Drive/datasets/texts/Тексты писателей.zip' -d /content/texts

def readText(fileName):          
  f = open(fileName, 'r')        
  text = f.read()                
  text = text.replace("\n", " ") 
  return text                   

className = ["О. Генри", "Стругацкие", "Булгаков", "Саймак", "Фрай", "Брэдберри"]
nClasses = len(className)

# загружаем обучающие тексты
trainText = []  # формируем обучающие тексты
testText = []   # формируем тестовые тексты

for i in className:              # проходим по каждому классу
  for j in os.listdir('texts/'): # проходим по каждому файлу в папке с текстами
    if i in j:                   # проверяем, содержит ли файл j в названии имя класса i
      
      if 'Обучающая' in j:                       # если в имени найденного класса есть строка "Обучающая" 
        trainText.append(readText('texts/' + j)) # добавляем в обучающую выборку
        print(j, 'добавлен в обучающую выборку') # выводим информацию
      if 'Тестовая' in j:                        # если в имени найденного класса есть строка "Тестовая"
        testText.append(readText('texts/' + j))  # добавляем в тестовую выборку
        print(j, 'добавлен в тестовую выборку')  # выводим информацию
  print()

maxWordsCount = 20000 
tokenizer = Tokenizer(num_words=maxWordsCount, filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff', 
                      lower=True, split=' ', oov_token='unknown', char_level=False)
tokenizer.fit_on_texts(trainText)          
items = list(tokenizer.word_index.items()) 
print(len(trainText))
print(len(trainText[0]))

# преобразовываем текст в последовательность индексов согласно частотному словарю
trainWordIndexes = tokenizer.texts_to_sequences(trainText) # обучающие тексты в индексы
testWordIndexes = tokenizer.texts_to_sequences(testText)   # проверочные тексты в индексы

print("Взглянем на фрагмент обучающего текста:")
print("В виде оригинального текста:              ", trainText[0][:87])
print("Он же в виде последовательности индексов: ", trainWordIndexes[1][:20], '\n')
print(len(trainWordIndexes))    # количество элементов в trainText
print(len(trainWordIndexes[0]))

def getSetFromIndexes(wordIndexes, xLen, step):
  xSample = []
  wordsLen = len(wordIndexes)
  index = 0
  
  # идём по всей длине вектора индексов
  # "откусываем" векторы длины xLen и смещаемся вперёд на step
  
  while (index + xLen <= wordsLen):
    xSample.append(wordIndexes[index:index+xLen])
    index += step
    
  return xSample

# функция принимает последовательность индексов, размер окна, шаг окна
def createSetsMultiClasses(wordIndexes, xLen, step):

  # для каждого из 6 классов создаём обучающую/проверочную выборку из индексов
  nClasses = len(wordIndexes)
  classesXSamples = [] 
  for wI in wordIndexes: 
    classesXSamples.append(getSetFromIndexes(wI, xLen, step))

  # формируем один общий xSamples
  xSamples = []
  ySamples = []
  
  for t in range(nClasses):   
    xT = classesXSamples[t]   
    for i in range(len(xT)):  
      xSamples.append(xT[i])  
    
    # формируем ySamples по номеру класса
    currY = utils.to_categorical(t, nClasses)
    for i in range(len(xT)):  
      ySamples.append(currY)  
 # переводим в массив numpy для подачи в нейронку
  xSamples = np.array(xSamples)
  # переводим в массив numpy для подачи в нейронку
  ySamples = np.array(ySamples) 
  # функция возвращает выборку и соответствующие векторы классов
  return (xSamples, ySamples)

# задаём базовые параметры
xLen = 1000 # длина отрезка текста, по которой анализируем, в словах
step = 100  # шаг разбиения исходного текста на обучающие вектора

# формируем обучающую и тестовую выборку
xTrain, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)
xTest, yTest = createSetsMultiClasses(testWordIndexes, xLen, step)
print(xTrain.shape)
print(yTrain.shape)
print(xTest.shape)
print(yTest.shape)

# Embedding + Dense сеть
model = Sequential()
model.add(Embedding(maxWordsCount, 50, input_length = xLen))
model.add(SpatialDropout1D(0.2))

model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(6, activation='sigmoid'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])
model.summary()

history = model.fit(xTrain, 
                      yTrain, 
                      epochs=15,
                      batch_size=32,
                      validation_data=(xTest, yTest))

plt.plot(history.history['accuracy'], 
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'], 
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()
plot_model(model, show_shapes=True, show_layer_names=True)

# Embedding + LSTM сеть
model1 = Sequential()
model1.add(Embedding(maxWordsCount, 10, input_length = xLen))
model1.add(SpatialDropout1D(0.2))
model1.add(BatchNormalization())
model1.add(LSTM(16, activation='relu', return_sequences=True))
model1.add(LSTM(16, activation='relu', return_sequences=True))
model1.add(Flatten())
model1.add(BatchNormalization())
model1.add(Dense(6, activation='softmax'))

model1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])   
model1.summary()

history = model1.fit(xTrain, 
                    yTrain, 
                    epochs=5,
                    batch_size=128,
                    validation_data=(xTest, yTest))

# строим график для отображения динамики обучения и точности предсказания сети
plt.plot(history.history['accuracy'], 
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'], 
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plot_model(model1, show_shapes=True, show_layer_names=True)
plt.show()

# Embedding + Conv1D сеть
model2 = Sequential()
model2.add(Embedding(maxWordsCount, 30, input_length = xLen))

model2.add(Conv1D(256, 5, activation='relu'))
model2.add(MaxPooling1D(5))
model2.add(Dropout(0.2))
model2.add(BatchNormalization())

model2.add(Conv1D(128, 5, activation='relu'))
model2.add(MaxPooling1D(5))
model2.add(Dropout(0.2))
model2.add(BatchNormalization())

model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(6, activation='sigmoid'))
model2.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])
model2.summary()

history = model2.fit(xTrain, 
                      yTrain, 
                      epochs=15,
                      batch_size=32,
                      validation_data=(xTest, yTest))

plt.plot(history.history['accuracy'], 
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'], 
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plot_model(model2, show_shapes=True, show_layer_names=True)
plt.show()

# Embedding + сложная сеть из Dense, LSTM и Conv1D
model3 = Sequential()
model3.add(Embedding(maxWordsCount, 50, input_length=xLen))
model3.add(SpatialDropout1D(0.2))

model3.add(Conv1D(64, 3, padding='same', activation="relu"))
model3.add(MaxPooling1D(2))
model3.add(Dropout(0.2))
model3.add(BatchNormalization())

model3.add(Conv1D(64, 3, padding='same', activation="relu"))
model3.add(MaxPooling1D(2))
model3.add(Dropout(0.2))
model3.add(BatchNormalization())

model3.add(Conv1D(64, 3, padding='same', activation="relu")) # добавляем одномерный сверточный слой, указывая кол-во фильтров и ширину окна для фильтров 
model3.add(MaxPooling1D(2))
model3.add(Dropout(0.2))                     # добавляем слой регуляризации, "выключая" указанное количество нейронов, во избежание переобучения
model3.add(BatchNormalization())             # добавляем слой нормализации данных

model3.add(LSTM(8, return_sequences=1))      # добавляем слой LSTM, совместимый с Cuda при поддержке GPU
model3.add(Dropout(0.2))                     # добавляем слой регуляризации, "выключая" указанное количество нейронов, во избежание переобучения
model3.add(BatchNormalization())             # добавляем слой нормализации данных

model3.add(Dense(256, activation='relu'))    # добавляем полносвязный слой с указанием количества нейронов и функции активации
model3.add(Dropout(0.2))
model3.add(Flatten())                        # добавляем слой выравнивания/сглаживания ("сплющиваем" данные в вектор)

model3.add(Dense(6, activation='softmax'))   # добавляем полносвязный слой на 6 нейронов, с функцией активации softmax на выходном слое

# компиляция, составление модели с выбором алгоритма оптимизации, функции потерь и метрики точности
model3.compile(optimizer='rmsprop', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
model3.summary()

history = model3.fit(xTrain, 
                    yTrain, 
                    epochs=10,
                    batch_size=64,
                    validation_data=(xTest, yTest))

# строим график для отображения динамики обучения и точности предсказания сети
plt.plot(history.history['accuracy'], 
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'], 
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plot_model(model3, show_shapes=True, show_layer_names=True)
plt.show()