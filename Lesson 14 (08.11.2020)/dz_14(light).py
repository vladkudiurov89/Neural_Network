# -*- coding: utf-8 -*-
"""DZ-14(Light).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n9pRx0A-9FBNkS4kCt6qGtSPwg4QGsUa
"""

# установка лемматизатора pymorphy2
!pip install pymorphy2

# Загружаем библиотеки
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Dense, Embedding, Input, concatenate, Activation, \
MaxPooling1D, Conv1D, BatchNormalization, Dropout, Conv2DTranspose, Lambda, Conv1DTranspose
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam, Adadelta
from tensorflow.keras import utils
from gensim.models import word2vec

import numpy as np
import pandas as pd
import pymorphy2
import re
import os
import time
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# Подключаем диск
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

# Чтение файла текста из файла, очитска от знаков препинания
def readText(fileName):
  f = open(fileName, 'r')
  text = f.read()
  # Определяем символы для удаления
  dS = ['\n', "\t", "\ufeff", ".", "_", "-", ",", "!", "?", "–", "(", ")", "«", "»", "№", ";"]

# Каждый символ в списке символов для удаления - yдаляем, заменяя на пробел
  for ds in dS: 
    text = text.replace(ds, '')

  text = re.sub('[.]', '', text)
  text = re.sub(":", " ", text)
  text = re.sub("<", " <", text)
  text = re.sub(">", "> ", text)

# Метод split разделит текст по пробелам, при помощи метода join запишем все разделенные слова снова в строку
  text = ' '.join(text.split()) 
  text = text.lower()
  return text

# Преобразование исходного текста в список из слов в нормальной форме 
def text2Words(text):
  # Создаем экземпляр класса MorphAnalyzer
  morph = pymorphy2.MorphAnalyzer()
  # Разделяем текст на пробелы
  words = text.split(' ')
  # Переводим каждое слово в нормальную форму 
  words = [morph.parse(word)[0].normal_form for word in words]  
  return words

# Путь к папке с договорами
directory = '/content/drive/My Drive/datasets/texts/Договора432/'
# Выведем названия 5-ти договоров
os.listdir(directory)[250:255]

# Список, в который запишем все наши договоры
agreements = [] 
# Проходим по всем файлам в директории договоров
for filename in os.listdir(directory): 
  # Читаем текст договора
  txt = readText(directory + filename) 
  # Если текст не пустой
  if txt != '': 
    # Преобразуем файл в одну строку и добавляем в agreements
    agreements.append(readText(directory + filename))

# Здесь будут храниться все договора в виде списка слов
words = [] 
# Засечем текущее время
curTime = time.time() 
# Проходимся по всем договорам
for i in range(len(agreements)): 
  # Преобразуем очередной договор в список слов и добавляем в words
  words.append(text2Words(agreements[i])) 
print('На преобразование ушло:', round(time.time() - curTime, 2), 'с.')

# Возьмем 10 текстов для финальной проверки обученной нейронной сети
wordsToTest = words[-10:]  
# Для обученающей и проверочной выборок возьмем все тексты, кроме последних 10
words = words[:-10]

print('Количество текстов в выборке для финальной проверки в конце ноутбука:', len(wordsToTest)) 
print('Количество договоров, взятых для обучающей и тестовой выборки:',len(words)) 
print('Количество слов в первом договоре:', len(words[0]))
print('Пример исходного текста:')
print(agreements[4][:62], '\n')
print('Тот же текст, представленный в виде списка из слов в начальной форме:')
print(words[4][:10])

# lower=True - приводим слова к нижнему регистру
# char_level=False - просим токенайзер учитывать слова, а не отдельные символы
tokenizer = Tokenizer(lower=True, filters = '', char_level=False)
 # "Скармливаем" наши тексты, т. е. даём в обработку методу, который соберет словарь частотности
tokenizer.fit_on_texts(words)
clean_voc = {} 
#Преобразуем полученный список
for item in tokenizer.word_index.items():
# В словарь, меняя местами элементы полученного кортежа   
  clean_voc[item[0]] = item[1]

print('Словарь всех слов по частоте:') 
print(clean_voc, '\n')
print('Длина словаря:', len(clean_voc))

tag = '<s1>'
print('Индекс тега', tag, ':' ,clean_voc[tag])

# Преобразовываем текст в последовательность индексов согласно частотному словарю
# Обучающие тесты в индексы
tok_agreem = tokenizer.texts_to_sequences(words) 

print("Взглянем на фрагмент обучающего текста:")
print("В виде оригинального текста:              ", words[4][:20])
print("Он же в виде последовательности индексов: ", tok_agreem[4][:20], '\n')

# Собираем список индексов и dummy encoded вектора
def get01XSamples(tok_agreem, tags_index):
  tags01 = [] # Список для тегов
  indexes = [] # Здесь будут лежать индексы
 
  for agreement in tok_agreem: # Проходимся по каждому договору-списку
    tag_place = [0, 0, 0, 0, 0, 0] # Создаем вектор [0,0,0,0,0,0]
    for ex in agreement: # Проходимся по каждому слову договора
        if ex in tags_index: # Смотрим, если индекс оказался нашим тегом
          place = np.argwhere(tags_index==ex) # Записываем, под каким местом лежит этот тег в своем списке
          if len(place)!=0: # Проверяем, чтобы тег действительно был
            if place[0][0]<6: # Первые шесть тегов в списке - открывающие
              tag_place[place[0][0]] = 1    # Поэтому ставим 1
            else: 
              tag_place[place[0][0] - 6] = 0  # Остальные теги закрывающие, так что меняем на ноль
        else:          
          tags01.append(tag_place.copy()) # Расширяем наш список с каждой итерацией. Получаем в конце длинный список из всех тегов в одном 
          indexes.append(ex) # Докидываем индекс слова в список индексов

  return indexes, tags01

# Получение списка слов из индексов
def reverseIndex(clean_voc, x):
  reverse_word_map = dict(map(reversed, clean_voc.items())) # Берем пары значений всего словаря и размечаем наоборот, т. е. value:key
  words = [reverse_word_map.get(letter) for letter in x] # Вытаскиваем по каждому ключу в список
  return words # Возвращаем полученный текст

# s1 Условия
# s2 Запреты
# s3 Стоимость, всё про цены и деньги
# s4 Всё про сроки
# s5 Неустойка
# s6 Всё про адреса и геолокации

tags_index = ['<s' + str(i) + '>' for i in range(1, 7)] # Получаем список открывающих тегов
closetags = ['</s' + str(i) + '>' for i in range(1, 7)] # Получаем список закрывающих тегов
tags_index.extend(closetags) # Объединяем все теги

tags_index = np.array([clean_voc[i] for i in tags_index]) # Получаем из словаря частотности индексы всех тегов
print('Индексы тегов:', tags_index)

# Распознаем теги и создаем список с ними, с индексами
xData, yData = get01XSamples(tok_agreem,tags_index)
# Для создания списков с embedding-ами сначала преобразуем список индексов обратно в слова
decoded_text = reverseIndex(clean_voc, xData)

print('Длина xData:', len(xData))
print('Длина yData:', len(yData))

print('Посмотрим на исходный набор слов: ', words[0][50:80])
print('Посмотрим на декодированный текст:', decoded_text[50:80])
print('Посмотрим на часть из xData:     ', xData[50:80])
print('Посмотрим на часть из yData:     ', yData[50:80])

# Формируем выборку из индексов
def getSetFromIndexes(wordIndexes, xLen, step): 
  xBatch = [] # Лист для фрагментов текста
  wordsLen = len(wordIndexes) # Определяем длину текста
  index = 0 # Задаем стартовый индекс
  
  while (index + xLen <= wordsLen): # Пока сумма индекса с длиной фрагмента меньше или равна числу слов в выборке
    xBatch.append(wordIndexes[index:index+xLen]) # Добавляем X в лист фрагментов текста
    index += step # Сдвигаемся на step

  return xBatch # Лист для фрагментов текста

xLen = 256 # Длина окна
step = 30 # Шаг 
embeddingSize = 300 # Количество измерений для векторного пространства

# Генерируем наборы с заданными параметрами окна
xTrain = getSetFromIndexes(decoded_text, xLen, step) # Последовательность из xLen слов
yTrain = getSetFromIndexes(yData, xLen, step) # Последовательность из xLen-тегов

print('Длина xTrain:', len(xTrain))
print('Длина yTrain:', len(yTrain))
print('Длина примера из xTrain:',len(xTrain[0]))
print('Длина примера из yTrain:',len(yTrain[0]), '\n')
print('Пример xTrain', xTrain[0])
print('Пример yTrain', yTrain[0], '\n')

print('Первый пример xTrain:', xTrain[0][step-5:step+5])
print('Второй пример xTrain:', xTrain[1][:10])

# Функция, которая смотрит на пересечение областей. Нужна для accuracy
def dice_coef(y_true, y_pred):
    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)

# Создаем выборки
def getSets(model, senI, tagI):
  xVector = [] # Здесь будет лежать embedding представление каждого из индексов
  tmp = [] # Временный список
  for text in senI: # Проходимся по каждому тексту-списку
    tmp=[]
    for word in text: # Проходимся по каждому слову в тексте-списке
      tmp.append(model[word]) 

    xVector.append(tmp)

  return np.array(xVector), np.array(tagI)

# Передаем в word2vec списки списков слов для обучения
# size = embeddingSize - размер эмбеддинга
# window = 10 - минимальное расстояние между словами в эмбеддинге 
# min_count = 1 - игнорирование всех слов с частотой, меньше, чем 1
# workers = 10 - число потоков обучения эмбеддинга
# iter = 10 - число эпох обучения эмбеддинга

modelGENSIM = word2vec.Word2Vec(xTrain, size = embeddingSize, window = 10, min_count = 1, workers = 10, iter = 10)

xTrainGENSIM, yTrainGENSIM = getSets(modelGENSIM, xTrain, yTrain)
print('Размерность полученного xTrain:', xTrainGENSIM.shape)
print('Размерность полученного yTrain:', yTrainGENSIM.shape)

np.save('xTrainGENSIM.npy', xTrainGENSIM) #Сохраняем массивы в память колаба
np.save('yTrainGENSIM.npy', yTrainGENSIM) #Сохраняем массивы в память колаба

xTrainGENSIM = np.load('xTrainGENSIM.npy') #Загружаем массивы из памяти колаба
yTrainGENSIM = np.load('yTrainGENSIM.npy') #Загружаем массивы из памяти колаба

# Функция, выводящая точность распознавания каждой категории отдельно
def recognizeSet(tagI, pred, tags, length, value):
  total=0

  for j in range(6): # общее количество тегов
    correct=0
    for i in range(len(tagI)): # проходимся по каждому списку списка тегов
      for k in range(length): # проходимся по каждому тегу
        if tagI[i][k][j]==(pred[i][k][j]>value).astype(int): # если соответствующие индексы совпадают, значит сеть распознала верно
          correct+=1 
    print("Сеть распознала категорию '{}' на {}%".format(tags[j],round(100*correct/(len(tagI)*length), 2)))
    total += 100 * correct / (len(tagI)*length)
  print("Cредняя точность {}%".format(total/6))

# Преобразовываем текст в последовательность индексов согласно частотному словарю
tok_agreemTest = tokenizer.texts_to_sequences(wordsToTest) # Обучающие тесты в индексы

print("Взглянем на фрагмент тестового текста:")
print("В виде оригинального текста:              ", wordsToTest[4][:20])
print("Он же в виде последовательности индексов: ", tok_agreemTest[4][:20], '\n')

xDataTest, yDataTest = get01XSamples(tok_agreemTest,tags_index) # Распознаем теги и создаем список с ними, с индексами
decoded_text = reverseIndex(clean_voc, xDataTest) # Для создания списков с embedding-ами сначала преобразуем список индексов обратно в слова

print('Длина xDataTest:', len(xDataTest))
print('Длина yDataTest:', len(yDataTest))

print('Посмотрим на исходный набор слов: ', wordsToTest[0][50:80])
print('Посмотрим на декодированный текст:', decoded_text[50:80])
print('Посмотрим на часть из xDataTest:     ', xDataTest[50:80])
print('Посмотрим на часть из yDataTest:     ', yDataTest[50:80])

xLen = 256 # Длина окна
step = 30 # Шаг 
embeddingSize = 300 # Количество измерений для векторного пространства

# Генерируем наборы с заданными параметрами окна
xTest = getSetFromIndexes(decoded_text, xLen, step) # Последовательность из xLen слов
yTest = getSetFromIndexes(yDataTest, xLen, step) # Последовательность из xLen-тегов

print('Длина xTest:', len(xTest))
print('Длина yTest:', len(yTest))
print('Длина примера из xTest:',len(xTest[0]))
print('Длина примера из yTrain:',len(yTest[0]), '\n')
print('Пример xTest', xTest[0])
print('Пример yTest', yTest[0], '\n')

print('Первый пример xTest:', xTest[0][step-5:step+5])
print('Второй пример xTest:', xTest[1][:10])

# Передаем в word2vec списки списков слов для обучения
# size = embeddingSize - размер эмбеддинга
# window = 10 - расстояние между текущим и прогнозируемым словом в предложении
# min_count = 1 - игнорирование всех слов с частотой, меньше, чем 1
# workers = 10 - число потоков обучения эмбеддинга
# iter = 10 - число эпох обучения эмбеддинга

modelGENSIM = word2vec.Word2Vec(xTest, size = embeddingSize, window = 10, min_count = 1, workers = 10, iter = 10)

xTestGENSIM, yTestGENSIM = getSets(modelGENSIM, xTest, yTest)
print('Размерность полученного xTrain:', xTestGENSIM.shape)
print('Размерность полученного yTrain:', yTestGENSIM.shape)

tags = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6']
# s1 Условия
# s2 Запреты
# s3 Стоимость, всё про цены и деньги
# s4 Всё про сроки
# s5 Неустойка
# s6 Всё про адреса и геолокации

#Функция для создания сети UNET подобной сети, без конкатеции слоев
def my_net(k = 1, k_size = 3, num_classes = 6, input_shape= (30, 300)):
        
    img_input = Input(input_shape) 

    #1
    x = Conv1D(64 * k , k_size, padding='same')(img_input) 
    x = BatchNormalization()(x) 
    x = Activation('relu')(x)

    x = Conv1D(64 * k , k_size, padding='same')(x)  
    x = BatchNormalization()(x)     
    x = Activation('relu')(x) 

    x = MaxPooling1D()(x)

    #2
    x = Conv1D(128 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)  

    x = Conv1D(128 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = MaxPooling1D()(x)

    #3
    x = Conv1D(256 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)               
    x = Activation('relu')(x)                     

    x = Conv1D(256 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv1D(256 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    
    x = MaxPooling1D()(x)

    #4
    x = Conv1D(512 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x) 
    x = Activation('relu')(x)

    x = Conv1D(512 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv1D(512 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)      
    x = Activation('relu')(x) 

    # UP 2
    x = Conv1DTranspose(256, kernel_size=2, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x) 

    x = Conv1D(256 * k , k_size, padding='same')(x) 
    x = BatchNormalization()(x) 
    x = Activation('relu')(x)

    x = Conv1D(256 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    
    # UP 3
    x = Conv1DTranspose(128, kernel_size=2, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x) 

    x = Conv1D(128 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv1D(128 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    # UP 4
    x = Conv1DTranspose(64, kernel_size=2, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv1D(64 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv1D(64 * k , k_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv1D(num_classes, k_size, activation='sigmoid', padding='same')(x)

    model = Model(img_input, x) 

    model = Model(img_input, x)
    model.compile(optimizer=Adam(0.0025), 
                  loss='categorical_crossentropy',
                  metrics=[dice_coef])
  
    return model

model_my = my_net(input_shape=(xLen, embeddingSize))

history_my = model_my.fit(xTrainGENSIM, yTrainGENSIM, epochs=20, batch_size=64, validation_split = 0.2)

pred_my = model_my.predict(xTestGENSIM) # сделаем предсказание

recognizeSet(yTestGENSIM, pred_my, tags, xLen, 0.999)

model_my2 = my_net(k=0.5, input_shape=(xLen, embeddingSize)) #уменьшим количество нейронов в 2 раза от исходной
history_my2 = model_my2.fit(xTrainGENSIM, yTrainGENSIM, epochs=20, batch_size=64, validation_split = 0.2)
pred_my2 = model_my2.predict(xTestGENSIM)
recognizeSet(yTestGENSIM, pred_my2, tags, xLen, 0.999)

model_my3 = my_net(k=2, input_shape=(xLen, embeddingSize)) #увеличим количество нейронов в 2 раза от исходной
history_my3 = model_my3.fit(xTrainGENSIM, yTrainGENSIM, epochs=20, batch_size=64, validation_split = 0.2)
pred_my3 = model_my3.predict(xTestGENSIM)
recognizeSet(yTestGENSIM, pred_my3, tags, xLen, 0.999)

model_my4 = my_net(k=0.25, input_shape=(xLen, embeddingSize)) #уменьшим количество нейронов в 4 раза от исходной
history_my4 = model_my4.fit(xTrainGENSIM, yTrainGENSIM, epochs=20, batch_size=64, validation_split = 0.2)
pred_my4 = model_my4.predict(xTestGENSIM)
recognizeSet(yTestGENSIM, pred_my4, tags, xLen, 0.999)

model_my5 = my_net(k=0.5, k_size=2, input_shape=(xLen, embeddingSize))
history_my5 = model_my5.fit(xTrainGENSIM, yTrainGENSIM, epochs=20, batch_size=64, validation_split = 0.2)
pred_my5 = model_my5.predict(xTestGENSIM)
recognizeSet(yTestGENSIM, pred_my5, tags, xLen, 0.999)

model_my6 = my_net(k=0.5, k_size=4, input_shape=(xLen, embeddingSize))
history_my6 = model_my6.fit(xTrainGENSIM, yTrainGENSIM, epochs=20, batch_size=64, validation_split = 0.2)
pred_my6 = model_my6.predict(xTestGENSIM)
recognizeSet(yTestGENSIM, pred_my5, tags, xLen, 0.999)

