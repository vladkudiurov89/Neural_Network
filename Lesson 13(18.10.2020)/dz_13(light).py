# -*- coding: utf-8 -*-
"""DZ-13(LIGHT).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QfLZHs2w_G4ZsDWlq8h5qaWvOQkK28q
"""

from google.colab import files 
import numpy as np 

from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Input 
from tensorflow.keras.optimizers import RMSprop, Adadelta 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras import utils 
from tensorflow.keras.utils import plot_model 

import yaml

from google.colab import drive
drive.mount('/content/drive')

# Открываем файл с диалогами

corpus = open('/content/drive/My Drive/datasets/texts/Диалоги(рассказы).yml', 'r') # открываем файл с диалогами в режиме чтения
document = yaml.safe_load(corpus) # загружаем файл *глоссарий
conversations = document['разговоры'] # загружаем диалоги из файла и заносим в conversations 
print('Количество пар вопрос-ответ : {}'.format(len(conversations)))
print('Пример диалога : {}'.format(conversations[125]))

# Разбираем вопросы-ответы с проставлением тегов ответам

# Собираем вопросы и ответы в списки
# здесь будет список вопросов
questions = list() 
# здесь будет список ответов
answers = list() 


for con in conversations:
  if len(con) > 2 : 
    questions.append(con[0]) 
    replies = con[1:] 
    ans = '' 
    for rep in replies: 
      ans += ' ' + rep 
    answers.append(ans) 
  elif len(con)> 1: 
    questions.append(con[0]) 
    answers.append(con[1])

# Очищаем строки с неопределенным типов ответов
answersCleaned = list()
for i in range(len(answers)):
  if type(answers[i]) == str:
    answersCleaned.append(answers[i])
  else:
    questions.pop(i) 

# Сделаем теги-метки для начала и конца ответов
answers = list()
for i in range(len(answersCleaned)):
  answers.append( '<START> ' + answersCleaned[i] + ' <END>' )

# Выведем обновленные данные на экран
print('Вопрос : {}'.format(questions[200]))
print('Ответ : {}'.format(answers[200]))

# Подключаем керасовский токенизатор и собираем словарь индексов

tokenizer = Tokenizer()
tokenizer.fit_on_texts(questions + answers)
vocabularyItems = list(tokenizer.word_index.items())
vocabularySize = len(vocabularyItems)+1
print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))
print( 'Размер словаря : {}'.format(vocabularySize))

# Устанавливаем закодированные входные данные(вопросы)

tokenizedQuestions = tokenizer.texts_to_sequences(questions) 
maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) 
# Делаем последовательности одной длины, заполняя нулями более короткие вопросы
paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')

encoderForInput = np.array(paddedQuestions)
print('Пример оригинального вопроса на вход : {}'.format(questions[100])) 
print('Пример кодированного вопроса на вход : {}'.format(encoderForInput[100])) 
print('Размеры закодированного массива вопросов на вход : {}'.format(encoderForInput.shape)) 
print('Установленная длина вопросов на вход : {}'.format(maxLenQuestions))

# Устанавливаем раскодированные входные данные(ответы)

tokenizedAnswers = tokenizer.texts_to_sequences(answers)
maxLenAnswers = max([len(x) for x in tokenizedAnswers])
paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')

decoderForInput = np.array(paddedAnswers)
print('Пример оригинального ответа на вход: {}'.format(answers[100])) 
print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100][:30])) 
print('Размеры раскодированного массива ответов на вход : {}'.format(decoderForInput.shape)) 
print('Установленная длина ответов на вход : {}'.format(maxLenAnswers))

# Раскодированные выходные данные(ответы)

tokenizedAnswers = tokenizer.texts_to_sequences(answers)
for i in range(len(tokenizedAnswers)) : 
  tokenizedAnswers[i] = tokenizedAnswers[i][1:]
paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers , padding='post')

oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize)
decoderForOutput = np.array(oneHotAnswers)
print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100][:21]))  
print('Пример раскодированного ответа на выход : {}'.format(decoderForOutput[100][4][:21])) 
print('Размеры раскодированного массива ответов на выход : {}'.format(decoderForOutput.shape))
print('Установленная длина вопросов на выход : {}'.format(maxLenAnswers))

# Первый входной слой, кодер, выходной слой
encoderInputs = Input(shape=(11, ))
encoderEmbedding = Embedding(vocabularySize, 200,  mask_zero=True) (encoderInputs)
encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)
encoderStates = [state_h, state_c]

# Второй входной слой, декодер, выходной слой
decoderInputs = Input(shape=(13, ))
decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) 
decoderLSTM = LSTM(200, return_state=True, return_sequences=True)
decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)
decoderDense = Dense(vocabularySize, activation='softmax') 
output = decoderDense (decoderOutputs)

# Собираем тренировочную модель нейросети
model = Model([encoderInputs, decoderInputs], output)
model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')
print(model.summary()) 
plot_model(model, to_file='model.png', show_shapes=True)

"""20 Epochs"""

# Запустим обучение и сохраним модель
model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=20) 
# model.save( '/content/drive/My Drive/Models/model_20epochs(rms).h5' )

# Создаем рабочую модель для вывода ответов на запросы пользователя

def makeInferenceModels():
  encoderModel = Model(encoderInputs, encoderStates) 
  decoderStateInput_h = Input(shape=(200 ,)) 
  decoderStateInput_c = Input(shape=(200 ,)) 
  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c]
  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)
  decoderStates = [state_h, state_c] 
  decoderOutputs = decoderDense(decoderOutputs)
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)
  return encoderModel , decoderModel


# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов
def strToTokens(sentence: str): 
  words = sentence.lower().split()
  tokensList = list()
  for word in words:
    tokensList.append(tokenizer.word_index[word])
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')

# Устанавливаем окончательные настройки и запускаем модель
encModel, decModel = makeInferenceModels()
for _ in range(6):
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start']
  stopCondition = False 
  decodedTranslation = ''
  while not stopCondition :
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    
    sampledWordIndex = np.argmax(decOutputs, axis=-1) 
    sampledWord = None 
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: 
        decodedTranslation += ' {}'.format(word) 
        sampledWord = word 

    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True 

    emptyTargetSeq = np.zeros((1, 1)) 
    emptyTargetSeq[0, 0] = sampledWordIndex 
    statesValues = [h, c] 
  
  print(decodedTranslation[:-3]) 
  # 20 эпох не удачные и не точные ответы

"""30 ЭПОХ"""

# Запустим обучение и сохраним модель
model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=30) 
# model.save( '/content/drive/My Drive/Models/model_30epochs(rms).h5' )

# Создаем рабочую модель для вывода ответов на запросы пользователя

def makeInferenceModels():
  encoderModel = Model(encoderInputs, encoderStates) 
  decoderStateInput_h = Input(shape=(200 ,)) 
  decoderStateInput_c = Input(shape=(200 ,)) 
  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c]
  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)
  decoderStates = [state_h, state_c] 
  decoderOutputs = decoderDense(decoderOutputs)
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)
  return encoderModel , decoderModel


# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов
def strToTokens(sentence: str): 
  words = sentence.lower().split()
  tokensList = list()
  for word in words:
    tokensList.append(tokenizer.word_index[word])
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')

# Устанавливаем окончательные настройки и запускаем модель
encModel, decModel = makeInferenceModels()
for _ in range(6):
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start']
  stopCondition = False 
  decodedTranslation = ''
  while not stopCondition :
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    
    sampledWordIndex = np.argmax(decOutputs, axis=-1) 
    sampledWord = None 
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: 
        decodedTranslation += ' {}'.format(word) 
        sampledWord = word 

    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True 

    emptyTargetSeq = np.zeros((1, 1)) 
    emptyTargetSeq[0, 0] = sampledWordIndex 
    statesValues = [h, c] 
  
  print(decodedTranslation[:-3]) 
  # 30 эпох немного лучше чем 20,но еще не предел

"""50 ЭПОХ"""

# Запустим обучение и сохраним модель
model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=50) 
# model.save( '/content/drive/My Drive/Models/model_50epochs(rms).h5' )

# Создаем рабочую модель для вывода ответов на запросы пользователя

def makeInferenceModels():
  encoderModel = Model(encoderInputs, encoderStates) 
  decoderStateInput_h = Input(shape=(200 ,)) 
  decoderStateInput_c = Input(shape=(200 ,)) 
  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c]
  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)
  decoderStates = [state_h, state_c] 
  decoderOutputs = decoderDense(decoderOutputs)
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)
  return encoderModel , decoderModel


# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов
def strToTokens(sentence: str): 
  words = sentence.lower().split()
  tokensList = list()
  for word in words:
    tokensList.append(tokenizer.word_index[word])
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')

# Устанавливаем окончательные настройки и запускаем модель
encModel, decModel = makeInferenceModels()
for _ in range(6):
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start']
  stopCondition = False 
  decodedTranslation = ''
  while not stopCondition :
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    
    sampledWordIndex = np.argmax(decOutputs, axis=-1) 
    sampledWord = None 
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: 
        decodedTranslation += ' {}'.format(word) 
        sampledWord = word 

    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True 

    emptyTargetSeq = np.zeros((1, 1)) 
    emptyTargetSeq[0, 0] = sampledWordIndex 
    statesValues = [h, c] 
  
  print(decodedTranslation[:-3]) 
  # 50 эпох похоже на заклинание

"""100 ЭПОХ"""

# Запустим обучение и сохраним модель
model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=100) 
# model.save( '/content/drive/My Drive/Models/model_100epochs(rms).h5' )

# Создаем рабочую модель для вывода ответов на запросы пользователя

def makeInferenceModels():
  encoderModel = Model(encoderInputs, encoderStates) 
  decoderStateInput_h = Input(shape=(200 ,)) 
  decoderStateInput_c = Input(shape=(200 ,)) 
  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c]
  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)
  decoderStates = [state_h, state_c] 
  decoderOutputs = decoderDense(decoderOutputs)
  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)
  return encoderModel , decoderModel


# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов
def strToTokens(sentence: str): 
  words = sentence.lower().split()
  tokensList = list()
  for word in words:
    tokensList.append(tokenizer.word_index[word])
  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')

# Устанавливаем окончательные настройки и запускаем модель
encModel, decModel = makeInferenceModels()
for _ in range(6):
  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))
  emptyTargetSeq = np.zeros((1, 1))    
  emptyTargetSeq[0, 0] = tokenizer.word_index['start']
  stopCondition = False 
  decodedTranslation = ''
  while not stopCondition :
    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)
    
    sampledWordIndex = np.argmax(decOutputs, axis=-1) 
    sampledWord = None 
    for word , index in tokenizer.word_index.items():
      if sampledWordIndex == index: 
        decodedTranslation += ' {}'.format(word) 
        sampledWord = word 

    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:
      stopCondition = True 

    emptyTargetSeq = np.zeros((1, 1)) 
    emptyTargetSeq[0, 0] = sampledWordIndex 
    statesValues = [h, c] 
  
  print(decodedTranslation[:-3]) 
  # 100 эпох слабый словарный запас



