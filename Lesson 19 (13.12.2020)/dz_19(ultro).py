# -*- coding: utf-8 -*-
"""DZ-19(ULTRO).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GhasXN9qqOFaEZqix6ZZ0yZTTB-UMz50
"""

!pip install pandas-datareader

pip install tensorflow==2.3.0

# Проверяем версию
import tensorflow as tf
tf.__version__

# Commented out IPython magic to ensure Python compatibility.
# Загружаем библиотеки
import math
import random
import pandas as pd
import pandas_datareader as data_reader
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from tqdm import tqdm_notebook, tqdm_notebook
from collections import deque
from tqdm import tqdm

from tensorflow.keras.layers import  Dense, BatchNormalization, Embedding, Flatten, Activation
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import utils

sns.set()
# %matplotlib inline

# Создадим класс Трейдер
class TraderTest():

  # Инициализируем класс и устанавливаем параметры
  def __init__(self, state_size, action_space=3, model_name="AITrader"):
      self.state_size = state_size
      self.action_space = action_space
      self.memory = deque(maxlen=2000)
      self.inventory = []
      self.model_name = model_name
      self.gamma = 0.95
      self.epsilon = 1.0
      self.epsilon_final = 0.01
      self.epsilon_decay = 0.995
  
  # Создадим модель из Dense слоев
  def get_create_model(self):
      model = Sequential()
      model.add(Dense(units=32, activation='relu', input_dim=self.state_size))
      model.add(Dense(units=64, activation='relu'))
      model.add(Dense(units=self.action_space, activation='linear'))
      model.compile(loss='mse', optimizer=Adam(lr=0.001))
      model.summary()
      return model

    # Проверка модели 
  def get_trade(self, state):
      if random.random() <= self.epsilon:
          return random.randrange(self.action_space)
      actions = self.model.predict(state)
      return np.argmax(actions[0])

    # Обучение модели
  def get_train(self, batch_size):   
      batch = []
      for i in range(len(self.memory) - batch_size + 1, len(self.memory)):
          batch.append(self.memory[i])
      for state, action, reward, next_state, done in batch:
        reward = reward
        q_optimal=reward
        if not done:
          q_optimal = reward + self.gamma * np.amax(self.model.predict(next_state)[0])           
        target = self.model.predict(state)    
        target[0][action] = q_optimal             
        self.model.fit(state, target, epochs=1, verbose=0)  
      if self.epsilon > self.epsilon_final:
          self.epsilon *= self.epsilon_decay

# Проверим тензор
tensor=tf.constant(np.arange(3).reshape(1,3))
print(tensor[0])
np.random.choice(2, 10, p=[0.2, 0.8])

# Функция сигмоиды
def sigmoid(x):
  return 1 / (1 + math.exp(-x))

# Функция формата отображения цены 
def stocks_price_format(n): 
  if n < 0:
    return "- $ {0:2f}".format(abs(n))
  else:
    return "$ {0:2f}".format(abs(n))

# Функция загрузки и очистки данных
def dataset_loader(stock_name):
  dataset = data_reader.DataReader(stock_name, data_source="yahoo")
  start_date = str(dataset.index[0]).split()[0]
  end_date = str(dataset.index[-1]).split()[0]
  close = dataset['Close']
  return close

# Функция создания скользящего окна
def state_creator(data, timestep, window_size): 
  starting_id = timestep - window_size + 1
  if starting_id >= 0:
    windowed_data = data[starting_id:timestep+1]
  else:
    windowed_data = - starting_id * [data[0]] + list(data[0:timestep+1]) 
  state = []
  for i in range(window_size - 1):
    state.append(sigmoid(windowed_data[i+1] - windowed_data[i])) 
  return np.array([state])

# Загрузим данные
stock_name = "AAPL"
data = dataset_loader(stock_name)
data.head()

# Зададим гиперпараметры
episodes = 1000
batch_size = 32
data_samples = len(data) - 1
window_size = 10

# Создадим модель
model_test = TraderTest(window_size)
model_test.get_create_model()

# Запустим обучение
for episode in range(1, episodes + 1): 
  print("Эпизод: {}/{}".format(episode, episodes)) 
  state = state_creator(data, 0, window_size + 1)
  total_profit = 0
  model_test.inventory = []
  for t in tqdm(range(data_samples)): 
    action = model_test.get_trade(state)
    next_state = state_creator(data, t+1, window_size + 1)
    reward = 0
    # Покупка акций
    if action == 1:
      model_test.inventory.append(data[t])
      print("Цена покупки: ", stocks_price_format(data[t]))
    # Продажа акций 
    elif action == 2 and len(model_test.inventory) > 0:
      buy_price = model_test.inventory.pop(0)
      reward = max(data[t] - buy_price, 0)
      total_profit += data[t] - buy_price
      print("Модель продает акции: ", stocks_price_format(data[t]), "Выгода : " + stocks_price_format(data[t] - buy_price) )
      
    if t == data_samples - 1:
      done = True
    else:
      done = False
      
    model_test.memory.append((state, action, reward, next_state, done))
    state = next_state
    if done:
      print("########################")
      print("Итоговая выгода: {}".format(total_profit))
      print("########################")   
  #   if len(model_test.memory) > batch_size:
  #     model_test.get_train(batch_size)    
  # if episode % 10 == 0:
  #   model_test.model.save("ai_trader_{}.h5".format(episode))





